--- 
title: "Analysis Methods for Complex Data Structures: Spatial Data"
author: "Mark Green"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
---

# Overview {-}

The resource here is part of the University of Liverpool module:

DASC507 â€“ Advanced Biostatistics II: Analysis Methods for Complex Data Structures

Specifically, the resources contained here are for the 'Spatial Data' part of the module. In the following four sessions we will explain how to deal with spatial data, visualise them, and introduce some techniques for analysising spatial data.

## Learning outcomes {-}

1. Produce static and interactive visualisations of spatial data.
2. Identify clustering of point- and area-based data.
3. Extend regression-based approaches to incorporate spatial context.

## Teaching structure {-}

There are a total of eight sessions as part of the spatial data component of the module. This contains four lectures, which will be short talks introducing concepts and applications within each technique, and four supplementary practicals that will cover how to implement the same techniques within R. The resources here cover the practical sessions, although links to the lecture slides are also provided at the relevant places.

## Computational notebooks {-}

The materials for the practical sessions are embedded within R notebooks. Notebooks are interactive documents that allow for executionable code to be embedded within for running analyses (and presenting their outputs within the same document). They are helpful for teaching, since you can combine analytical code, the resulting output, and interpretation of what was done in a single file. All of the data and scripts are included within the folder structure, meaning that everything should be fully replicatable. Follow the documents along, reading the guidance and testing parts of the code. Feel free to edit the documents and code as you learn, so that you can have one single resource. 

Each package we use in the tutorials will need to be pre-installed. To install any package within R, please use `install.packages("")` and specify the package name in-between `""`. Good R practice is to load all dependencies/packages at the start of any script, however for the purpose of these tutorials we will load each as and when we need them so that you can see when and where they are required.

The course materials are all written using the **bookdown** package [@R-bookdown], which was built on top of R Markdown and **knitr** [@xie2015].

## Contact {-}

Dr Mark A. Green \
Senior Lecturer in Health Geography \
University of Liverpool \
<mark.green@liverpool.ac.uk>



<!--chapter:end:index.Rmd-->

# Mapping data {#intro}

In this section, we will introduce how to map and visualise spatial data in R. It will introduce you to the key R packages for wrangling and plotting spatial data, and demonstrate how to use them for different data types. The lecture slides for this practical can be found [here](). A PDF version of the practical can be found [here]().

## Why map?

Geography is the lens at which we view the World. Through exploring how health outcomes vary between neighbourhoods, cities or regions, as well as their drivers, we can start to piece together the underlying processes affecting health. Visualisations allow us to make data 'real' and can give us an intuitive sense of what is going on. Maps are data visualisations that have an inherent spatial dimension to them. 

## Mapping in R

There are several R packages that allow us to map and visualise data within R - too many to cover in just one module! Here we will focus on the three main packages for mapping vector data. Raster data are less common in public health, social science and health-related data science (satellite-derived measures, such as green space coverage or sensor estimated pollution, are the exceptions here). As such, we will not cover how to map them here. You can review the lecture slides over these data types [here](). In brief, vector data refer to points, lines or polygons (shapes representing areas)

First, we have `sf` (Simple Features) which provides allows R to engage with spatial data and therefore is key for any wrangling of spatial data we need to do. Through base R's `plot()` command, we can map data. We need `sf` for most spatial packages or commands, so it is core here. I find `plot()` somewhat restrictive in designing pretty or engaging visualisations. It is quick to run and requires minimal code, so often offers a 'first pass' at looking at the data

Second, `ggplot2` (Grammar of Graphics for Plotting v2) is one way of plotting data and producing professional-style visualisations. Plots are built sequentially, adding features one line at a time. It can be a bit 'fiddly' to learn, but once you understand how it works it can offer a great deal of customisation. We will focus on `ggplot2` for most of our tutorials.

Finally, `tmap` (Thematic Maps) is a package built primarily for mapping spatial data. The format follows a similar style to `ggplot2`. I find it easier to use for creating quick maps, but is less flexible than compared to `ggplot2` when changing specific aspects of plots. It is probably personal perference which you will end up using.

We will now go through how to use these packages to plot different types of vector data to ~~sneak in~~ introduce the key concepts of mapping. 

### Areas

We first start with mapping data for areas. This may include information on neighbourhoods, cities, regions or countries - much health data is made available for areas and so this is a useful way of visualising health data. Data on individuals are often aggregated to geographical zones to make sure they are less disclosive as well. 

Spatial data are not stored in common data formats we may have encountered elsewhere (e.g., csv or text files). They require formats that can recognise their spatial nature, so that we can plot their geographical patterns clearly. One of the main types of storing spatial data are shapefies (.shp and associated files). Other file type formats for vector data include .GeoJSON (Geographic JavaScript Object Notation), .gml (Geography Markup Language), .kml (Google Keyhole Markup Language) and .gpx (GPS eXchange Format). We will stick with shapefiles for the purpose of this tutorial. 

Let's have a look at some area data. We will load in small areas for Liverpool, specifically **Middle Super Output Areas (MSOAs)**. MSOAs are statistical zones ucreated for the purpose of mapping administrative data. They represent ~7500 individuals and therefore represent large neighbourhoods to small towns/large villages. For most Census-based UK shapefiles, we can find them at [UK Borders](https://borders.ukdataservice.ac.uk/). You can find the data themselves stored in the **Shapefiles** folder. We will also load the R package `sf` so that R can handle these data. 

```{r}
library(sf) # Load in sf package
msoas <- read_sf("./Shapefiles/Liverpool_MSOAs/england_msoa_2011.shp")  # Load in shapefile

```

You may have noticed that we only need to load in a single file - the shapefile - and not the other files in the folder. This is because they each store supplementary information that helps R know how to interpret the shapefile (e.g., the projection of the data).

Vector data are plotted based on their **coordinate reference system (CRS)**. The CRS gives the spatial location of the data we are plotting. These are typically points and through joining up points we can create lines or polygons. They usually have two values representing the 'x' (longitude - the East to West location with respect to the distance from the 'Prime Meridian') and 'y' (latitude - the North to South location with respect to distance from the 'Equator') values of where they are. 

How 'distance' is measured is important and is what the CRS tell us. As the World is ellipsoidal (it is not quite spherical), and our computer screens are flat we need to tell R how to convert these values between the two formats. Essentially, we need to use *projections* to convert the three-dimensional *geographic* location onto a two-dimensional device. This process leads to some distortion of area size, direction, distance or shape. Selecting an appropriate projection is therefore important for plotting data and avoiding misleading visualisations. Different countries have different preferred projections.

Fortunately, most spatial data is provided with a CRS. Using `sf` we can identify a CRS using `st_crs()`. 

```{r}
st_crs(msoas)

```

You can see that the CRS used is 1936 British National Grid - a common CRS for files from Great Britain. To re-project any spatial data to a specific CRS, we can use the following command `st_transform()`.

Let's plot the spatial data usig `sf` and `plot()`.

```{r}
plot(msoas)

```

Here we can see each MSOA's location in Liverpool. Three maps are presented - one for each column in the shapefile. This is one reason why `plot()` is not useful, since it will just plot everything. Here is it is just plotting the descriptors of each MSOA (name = name of MSOA, code/label = unique code of MSOA). You can inspect the data yourself through using `head(msoas)`.

Mapping the MSOAs themselves is a little boring, so let's join on some data. We will map COVID-19 vaccine uptake data for Liverpool - specifically the numbers of people who had receiveda vaccination between 8th December 2020 to 6th June 2021. The data are open and can be found [here](https://www.england.nhs.uk/statistics/statistical-work-areas/covid-19-vaccinations/). I have cleaned the original file and saved it in **Data** folder. The variables we have are:

* msoa_code - unqiue code for each MSOA
* msoa_name - the name of each area
* total_first_dose - the number of people who received their first COVID-19 vaccine dose
* total_second_dose - the number of people who received two COVID-19 vaccine doses

We will now load in the data and join it onto the shapefile. To do the latter step, we will need to join on a common identifier - in this case MSOA code as it is unique and allows us to assign a one-to-one match between datasets. The `merge` command can join together files, as long as we tell R on what files to join based on (i.e., which columns are the MSOA codes found).

```{r}
vaccine_uptake <- read.csv("./Data/msoa_vaccine_10June21.csv") # Load in vaccine uptake data
msoas <- merge(msoas, vaccine_uptake, by.x = "code", by.y = "msoa_code", all.x = TRUE) # Merge the vaccine uptake objects onto the msoa object, based on the columns defined (x = msoas, y = vaccine_uptake), and do this for all observations only in the x (msoas) object
  
```

You should always quickly check whether it worked. I usually run a brief `head(msoas)` check to see if it works.

First, we will plot the number of people who have received their first dose of the COVID-19 vaccine. We will do this using `ggplot2`

```{r}
library(ggplot2) # Load in package
map1 <- ggplot() + # Call ggplot command
          geom_sf(data = msoas, aes(fill = total_first_dose)) # Using a spatial object, plot MSOAs and fill in based on number of people with first COVID-19 dose
map1 # Print plot

```

Well done - you've made your first map! What patterns can you see? What might explain these patterns?

We can edit the map to make it nicer, how about trying the following options. Remember that `ggplot` adds each feature line-by-line, so the order of your code sometimes matters to how it is plotted.

```{r}
library(viridis) # For colour blind friendly colours
map1 <- ggplot() + # Call ggplot command
            geom_sf(data = msoas, aes(fill = total_first_dose), lwd = 0) + # Using a spatial object, plot MSOAs and fill in based on number of people with first COVID-19 dose, with line width = 0 (i.e., not visible)
            scale_fill_viridis_c(option = "plasma") + # Make colour-blind friendly
            xlab("Longitude") + # Add x-axis label
            ylab("Latitude") + # Add y-axis label
            labs(title = "First dose COVID-19 vaccinations", # Add title to map
                 fill = "Frequency") # Edit legend title
map1 # Print plot

```

Why not try editing some of the values above and see how it changes the aestheics of the map.

To save the map, we use the following.

```{r}
ggsave(plot = map1, filename = "./Plots/map1_ggplot.jpeg", dpi = 300) # save

```

We next move onto plotting using `tmap`. Here we will plot the number of people who have had two doses of the COVID-19 vaccine.

```{r}
library(tmap) # Load package
map2 <- tm_shape(msoas) + # Call which spatial object
          tm_polygons("total_second_dose") # Which column to plot
map2 # Print

```

Just as easy to use! Again, let's make the map prettier. We can also add in a few other common map conventions including a north arrow and a scale bar far easier with tmap. Note: calling viridis here can be a bit temperamental, and may need you to reload the package before running the code.

```{r}
map2 <- tm_shape(msoas) + # Call which spatial object
          tm_polygons("total_second_dose", palette = "viridis", title = "Frequency") + # Which column to plot and plot using colour blind friendly colours and edit legend to 'Frequency'
          tm_layout(main.title = "Second dose uptake") + # Add title
          tm_scale_bar(position = c("right", "top"), width = 0.15) + # Add scale bar
          tm_compass(position = c("left", "top"), size = 2) # Add north arrow
map2 # Print
  
```

To save the map, we do the following.

```{r}
tmap_save(map2, filename = "./Plots/map2.jpeg", dpi = 300) # Save file

```

And there you go - just like that you have learned how to plot in R using three different packages! Easy.

### Points

The next type of data we will consider are points. The area polygons we have just mapped are really a bunch of points connected by lines, so they are the building blocks of spatial data. However, they offer value by themselves for representing specific spatial positions (e.g., locations of health services).

Point data may be supplied in a spatial data format, or can just be provided as a text/csv file with a list of spatial points recorded within. The latter format can make them a little easier to handle, although since they are missing their CRS we will need to define it ourselves.

Let's have a go at plotting some points. We will stick with the COVID-19 vaccination theme here. During the initial roll-out of vaccines, Liverpool City Council funded a 'vaccination bus' that could travel around Liverpool and make getting a vaccination more accessible (i.e., people could just turn up and get vaccinated there and then). Let's have a look at the areas they brought the bus to. 

To load in the point locations of where the bus traveled to, we can treat the data as a standard data frame in R.

```{r}
bus_locations <- read.csv("./Data/liverpool_bus_locations.csv") # Load data
  
```

If we use `head(bus_locations)`, we can inspect the data. It includes the following variables:

* site - name of the location visited
* longitude - the spatial location, as measured east-west of the Greenwich Meridian point
* latitude - the spatial location, as measured north-south of the Equator
* postcode - the postal address of the site (note that postcodes are not unique and may represent 15 households)
* location - latitude and longitude combined into a single geometry point

The data is currently stored in a data frame format. We need to tell R that it is actually spatial data, so that it can plot it as such. `sf` can help us here.

```{r}
# Convert to spatial data frame
bus_locations_sp <- bus_locations %>% # For object (bus locations)
  st_as_sf(coords = c("longitude", "latitude")) %>% # Define as spatial object and identify which columns tell us the position of points
  st_set_crs(4326) # Set CRS

```

Let's look at where these points are located. First, we will use `ggplot2`.

```{r}
map3 <- ggplot() + # Call ggplot command
            geom_sf(data = bus_locations_sp, colour = "green") + # Plot points as green dots
            xlab("Longitude") + # Add x-axis label
            ylab("Latitude") + # Add y-axis label
            labs(title = "Location of vaccine bus") # Edit legend title
map3 # Print plot

```

Hmmmm, that is not particularly useful by itself. Let's add these points to the map of vaccine uptake that we made earlier. This will help us tell if the locations were targeted at areas with low or high uptake. Note that we call the points after the area data, since `ggplot2` builds the plot sequentially, this helps to ensure the points are plotted on top of the area data.

```{r}
map3 <- ggplot() + # Call ggplot command
            geom_sf(data = msoas, aes(fill = total_first_dose), lwd = 0) + # Plot vaccine uptake (1st dose)
            scale_fill_viridis_c(option = "plasma") + # Make colour-blind friendly
            geom_sf(data = bus_locations_sp, colour = "green") + # Plot points as green dots
            xlab("Longitude") + # Add x-axis label
            ylab("Latitude") + # Add y-axis label
            labs(title = "First dose COVID-19 vaccinations", # Add title to map
                 caption = "Green dots are locations of vaccine bus", # Add description to bottom of map (alternatively could place as subtitle = "" too)
                 fill = "Frequency") # Edit legend title
map3 # Print plot

```

How useful do you think the vaccine bus locations are? Do you think this approach is helpful? How might you improve or better target their locations?

We can do the same using `tmap` too. We next recreate the same map from earlier (2nd dose uptake) and compare it to vaccine bus locations. 

```{r}
map4 <- tm_shape(msoas) + # Call area data object
          tm_polygons("total_second_dose", palette = "viridis", title = "Frequency") + # Which column to plot and plot using colour blind friendly colours and edit legend to 'Frequency'
        tm_shape(bus_locations_sp) + # Call spatial points
          tm_dots(size = 0.5) + # Plot points (cal call specific variable here too)
        tm_layout(main.title = "Second dose uptake") + # Add title
        tm_scale_bar(position = c("right", "top"), width = 0.15) + # Add scale bar
        tm_compass(position = c("left", "top"), size = 2) # Add north arrow
map4 # Print

```

Here, we have to call two spatial objects and then tell `tmap` what to plot, which is a different approach to the `ggplot2` code.

## Interactive maps

So far we have created 'static' maps. They are static in the sense that they do not move or change, which is important for print media (i.e., those printed in articles). It can be preferable to create interactive plots where users can actively engage with data. 

`ggplot2` does not natively allow for interactive maps, however `tmap` does. All we have to do is tell `tmap` we want to create an interactive and clickable map, and then run our code from earlier.

```{r}
tmap_mode("view") # Set tmap to interactive plotting mode
map4 # Plot map object
# Alternatively, you can just run the code and it will create the interactive map on the fly - try running the following:
# tm_shape(msoas) + # Call which spatial object
#   tm_polygons("total_second_dose", palette = "RdYlBu") # Which column to plot, and change colour

```

If you hover over points or areas, you will see there is a description of what they are (based on first column in the dataset, although this can be changed). You can also change the base map by clicking on the layers box to the left. This is also useful as you can plot multiple layers at once and switch between them.

If you want to make static maps again, you will need to tell `tmap` this through running the following code `tmap_mode("plot")`.

We can also save these interactive maps as standalone html files that can be shared.

```{r}
tmap_save(map4, filename = "./Plots/vaccine_bus_interactive.html") # Save

```

We can edit the basemaps presented here to a range of options. A couple of my favourites include:

* Carto as basemap for tiles, rather than default of ESRI
* Stamen which is just pretty
* OpenStreetMap is always worth a shout

Below is some code to try these styles, but you can inspect the range of designs and options [here](https://leaflet-extras.github.io/leaflet-providers/preview/) 

```{r}
# # Carto - light
# tm_basemap(leaflet::providers$CartoDB.PositronNoLabels, group = "CartoDB basemap") + # Plot Carto basemap
#     tm_shape(msoas) + # Select MSOA object
#     tm_polygons("total_second_dose", palette = "RdYlBu") + # Plot vaccine uptake (2nd dose)
#     tm_tiles(leaflet::providers$CartoDB.PositronOnlyLabels, group = "CartoDB labels") # Plot place name labels

# # Carto - dark
# tm_basemap(leaflet::providers$CartoDB.DarkMatter) + # Plot Carto basemap
#     tm_shape(msoas) + # Select MSOA object
#     tm_polygons("total_second_dose", palette = "RdYlBu")

# # Stamen
# tm_basemap("Stamen.Watercolor") + # Add Stamen as basemap
#     tm_shape(msoas) + # Select MSOA object
#     tm_polygons("total_second_dose", palette = "RdYlBu") + # Plot vaccine uptake (2nd dose)
#     tm_tiles("Stamen.TonerLabels") # Adds labels for areas (e.g., place names)

# # OpenStreetMap
# tm_basemap("OpenStreetMap.HOT") + # Plot basemap
#     tm_shape(msoas) + # Select MSOA object
#     tm_polygons("total_second_dose", palette = "RdYlBu")

```

The eagled-eye of you may have noticed that this technology is enabled through something called `leaflet`. We can actually create interactive maps using `leaflet` directly, which gives greater flexibility and control (even if the code is longer and more difficult). Let's give it a go!

```{r}
# Load package
library(leaflet)

# Transform the CRS to match WGS84 format for leaflet
msoas <- st_transform(msoas, 4326)

# Define parameters for colours for mapping
pal <- colorNumeric(viridis_pal(option = "viridis")(2), domain = c(0, 5000)) # Set doman as min/max

# Plot
leaflet() %>%
  # Add area data on vaccine uptake populations
  addPolygons(data = msoas, # Define data
              fillColor = ~pal(total_second_dose), # Specify the variable to be plotted, with pal representing the colours to plot
              weight = 0, # Define how thick the lines will be 
              opacity = 0, # How see through we want the lines to be (0%)
              fillOpacity = 0.5) %>% # How see through we want areas coloured in to be (50%)
  # Add bus locations in
  addCircleMarkers(data = bus_locations_sp, # Define data
                   radius = 1, # Only plot immediate location
                   color = "red") %>% # Select colour to plot
  addProviderTiles("CartoDB.Positron") # Define base map, here as Carto

```

## Summary

Well done! You have learned how to visualise spatial data, make professionally looking maps, and think about how to effectively present information in static and interactive ways. In the next section, we will consider how to apply this spatial way of presenting data into more formal approaches of analysis. 

<!--chapter:end:01-mapping-data.Rmd-->

# Cluster analysis {#cluster}

In this section, we will introduce how we can analyse spatial data including identifying clustering of data. The lecture slides for this practical can be found [here](). A PDF version of the practical can be found [here](). We define clustering here as where data (locations, or high/low values) are geographically concentrated in particular areas and not evenly spread out.

## Point data

There are numerous point-based methods including clustering algorithms, spatial scan metrics, and count regression models that we could talk about here - too much to cover in this section. Instead, we will focus on descriptive techniques for displaying point-based data.

### Spatial histograms

One of the first steps we might take in trying to see if our point data are spatially clustered is by plotting their locations. This may be important if we are looking at the locations of disease cases or outbreaks, allowing us to make inferences about clustering of diseases and the reasons behind this. In the example here, we will look at food outlets in Liverpool to assess if they are geographically concentrated in particular areas of Liverpool.

We will first load in the data on locations of food outlets. The data are taken from the Food Standards Agency website [here](https://ratings.food.gov.uk/) and are open data. While there are too many columns to describe here, it is worth familiarising yourself with all these columns using `head(food_outlets)` first. The key variables we will be using are:

* Geocode.Longitude - longitude for location of premises
* Geocode.Latitude - latitude for location of premises

We also will load in the boundary shapefile for Liverpool here, to help provide a background for our points.

```{r}
library(sf) # Load package

# Load food outlets data
food_outlets <- read.csv("./Data/liverpool_food_outlets.csv") # Load data
food_outlets <- food_outlets[!is.na(food_outlets$Geocode.Latitude),] # Drop missing co-ordinates (e.g., burger vans that have no fixed position) since cannot plot them

# Convert to spatial points data frame
food_outlets_sp <- food_outlets %>% # For object
  st_as_sf(coords = c("Geocode.Longitude", "Geocode.Latitude")) %>% # Define as spatial object and identify which columns tell us the position of points
  st_set_crs(4326) # Set CRS

# Load Liverpool outline
liverpool <- read_sf("./Shapefiles/Liverpool_LAD/england_lad_2011.shp")  # Load in shapefile
liverpool <- st_transform(liverpool, 4326) # Reproject to same as long/lat format

```

Ok, with the data loaded in, we can now map the points.

```{r}
library(ggplot2) # Load package
map1 <- ggplot() + # Call ggplot command
  geom_sf(data = liverpool) + # Load liverpool outline
  geom_sf(data = food_outlets_sp) + # Plot food outlet as dots
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Food outlets in Liverpool") # Edit plot title
map1 # Print plot

```

Hmmmm, you may see there is an issue with interpretating this plot - many of the plots overlap each other making it difficult to determine underlying patterns. We therefore need a way of summarising this information. 

Histograms are useful plots for examining the distribution of single variables - we can apply them spatially by creating **spatial histograms**. To do this, we need to create spatial **bins** and then count how many points are in each of those bins. We use this binning approach when producing histograms for single variables (e.g., counting how many points exist at each value of a variable) and just extend it here for each x,y position on a map. I recommend using a **hex** approach, since hexagons can minimise visual artefacts that can be created through other shapes.  

```{r}
library(viridis) # Load package
map2 <- ggplot() + # Call ggplot2
  geom_sf(data = liverpool) + # Load liverpool outline
  geom_hex(data = food_outlets, aes(x = Geocode.Longitude, y = Geocode.Latitude)) + # Define data to be plotted
  scale_fill_viridis() + # Make colour blind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Food outlets in Liverpool") # Edit plot title
map2

```

We can now see the clustering of points in the city centre, with food outlets covering most of the city. 

### Kernal Density Estimation

An alternative approach might be to use **Kernal Density Estimation (KDE)** which can create a continuous surface for the density/clustering of points. This avoids the limitation of spatial histograms through creating a single map with no boundaries, rather than splitting up areas into arbitrary bins. KDE or density plots are often used as alternative to standard histograms.

Here we are measuring the intensity or density of points for a given location. We can run these analyses in `ggplot2` using the `stat_density2d_filled` command. 

```{r}
map3 <- ggplot() + # Call ggplot2
    geom_sf(data = liverpool) + # Load liverpool outline
    stat_density2d_filled( # Call KDE or smoothed density plot
        data = food_outlets, aes(x = Geocode.Longitude, y = Geocode.Latitude, # Define data and locations
                                 fill = ..level..,alpha=..level..), # Set paramters for colouring in values
        n = 100 # Modify and see how changes the map (is number of neighbours to derive clustering from)
    ) +
    scale_color_viridis() + # Make colour blind friendly
    xlab("Longitude") + # Add x-axis label
    ylab("Latitude") + # Add y-axis label
    labs(title = "Food outlets in Liverpool") # Edit plot title
map3

```

The resulting map is similar to the hexmap earlier, but we now have one contiguous and single surface for the data. It is worth playing about with the number of neighbours to examine how the algorithm changes the resulting map. Sometimes these are called 'heatmaps'.

### Spatial interpolation

Sometimes the point data that we have contain values or variables that tell us information about them. Here we might be interested in values themselves, rather than the clustering of their locations. We therefore need a different approach here.

To demonstrate how to visualise and extract some value here, we will use a new dataset. The data can be found [here](https://qof.digital.nhs.uk/) and are open data. I have done some additional cleaning on the data and extracted their spatial locations to save us time. There is active debate over how useful these data are, but they provide a helpful case study. The variables in the dataset are:

* pcn_code - unique code for the primary care network (groups of GP practices/surgeries)
* pcn_name - name for primary care network
* gp_code - unique code for GP surgery
* gp_name - name of GP surgery
* chronic_heart_disease_percent - estimated percentage of people at the surgery who have chronic heart disease
* obese_percent - estimated percentage of people who are defined as obese
* postcode - postcode for GP surgery
* longitude - longitude location
* latitude - latitude location

Let's load in these data.

```{r}
# Load data
gp_locations <- read.csv("./Data/gp_qof_1920.csv")

# Convert to spatial points data frame
gp_locations_sp <- gp_locations %>% # For object
  st_as_sf(coords = c("longitude", "latitude")) %>% # Define as spatial object and identify which columns tell us the position of points
  st_set_crs(4326) # Set CRS

```

There are numerous ways that we might plot or visualise these data. First, we could simply assign colours to each point based on their values. We will explore this using the data on percentage of registered patients who are obese.

```{r}
map4 <- ggplot() + # Call ggplot2
  geom_sf(data = liverpool) + # Plot Liverpool outline
  geom_sf(data = gp_locations_sp, aes(color = obese_percent), size = 2) + # Define what to map (adjusted size to make easier to see)
  scale_color_viridis() + # Make colour blind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Percentage of patients who are obese", # Edit plot title 
       color = "Obesity (%)") # Edit legend title (note must match color as that is what we are plotting)
map4

```

This is a little hard to see general patterns, but follows what we learnt last week.

A second approach would be to adjust the size of the dots in relation to their value.

```{r}
map5 <- ggplot() + # Call ggplot2
  geom_sf(data = liverpool) + # Plot Liverpool outline
  geom_sf(data = gp_locations_sp, aes(size = obese_percent),) + # Define what to map
  scale_color_viridis() + # Make colour blind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Percentage of patients who are obese", # Edit plot title 
       size = "Obesity (%)") # Edit legend title (note must match size as that is what we are plotting)
map5

```

Adjusting the size of points can be visually more striking, but is also harder to interpret the exact values. It is easier for the human mind to judge differences in colour rather than differences in sizes of points. Try identifying locations that are grouped as 15 or 20 on the map - it's hard!

Since our maps only present data for the spatial points themselves, we may want to estimate what the data might be like inbetween points to help identify the area extent of clusters or general spatial patterns. Here, we would turn to **spatial interpolation** techniques that can create a continuous surface based on given point values. 

There are various spatial interpolation techniques that we can then use to fill in the gaps including inverse distance weighting or spatial kriging techniques. In this tutorial, we will just use non-linear **splines** to estimate values inbetween points. Splines are faster to run, but may give less precise results. 

The following code below runs the interpolation code and then sorts the data out to a format for mapping. We need a couple of new packages here - `akima` and `reshape2` - although, we will only use them briefly so I will not describe them in detail.

```{r}
# Interpolate data
library(akima) # Load package
obese_interp <- with(gp_locations, interp(x = longitude, y = latitude, z = obese_percent, duplicate = "mean", linear = FALSE)) # Set parameters for spatial location, what to predict, how to deal with duplicate values (i.e., take the mean), and linear = FALSE means uses cubic splines

# Data wrangling
library(reshape2) # Load package
pred_obese_df <- melt(obese_interp$z, na.rm = TRUE) # Convert predicted values from a list (in wide format) to data frame (in long format)
names(pred_obese_df) <- c("x", "y", "obese_percent") # Rename columns

# Sort out longitude values
x <- melt(obese_interp$x, na.rm = TRUE) # Convert longitude values from list format to data frame
x <- data.frame(row.names(x), x, row.names = NULL) # Save row values as column (these match up to melt values x)
names(x) <- c("x", "longitude") # Rename columns
pred_obese_df <- merge(pred_obese_df, x, by = "x", all.x = T) # Join on longitude values to their lookup values from the melt process
rm(x) # Tidy

# Sort out latitude values
y <- melt(obese_interp$y, na.rm = TRUE) # Convert longitude values from list format to data frame
y <- data.frame(row.names(y), y, row.names = NULL) # Save row values as column (these match up to melt values y)
names(y) <- c("y", "latitude") # Rename columns
pred_obese_df <- merge(pred_obese_df, y, by = "y", all.x = T) # Join on longitude values to their lookup values from the melt process
rm(y) # Tidy

```

Phew, that code was rather messy but it does get the job done (if you know a better way, let me know!). We can now plot it using a heat map.

```{r}
map6 <- ggplot() + # Call ggplot2
  geom_sf(data = liverpool) + # Plot Liverpool outline
  geom_tile(data = pred_obese_df, aes(x = longitude, y = latitude, fill = obese_percent)) + # Define what to map (interpolated values)
  scale_fill_viridis() + # Make colour blind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Estimated obesity prevalence", # Edit plot title 
       fill = "Obesity (%)") # Edit legend title (note must match fill as that is what we are plotting)
map6

```

We should be careful in producing these maps, since we are estimating data inbetween points and the resulting maps may not be correct. For example, it makes an assumption that people go to their nearest GP surgery to where they live, which is not always true. We can see a cluster of high levels of obesity north of the city centre, which is a densely populated deprived area. However, there is a lot of variability in the data and it is a little messy. Hopefully at least it gives you can idea of how to use these techniques and what they can produce.

Can you produce a similar map for chronic health disease prevalence? (tip: the variable required is `gp_locations$chronic_heart_disease_percent`).

## Area data

These techniques introduced for point-based data do not work with area data. However, we can utilise more powerful techniques to examine the existence of clustering in our data. 

We will utilise a new R package - `spdep` (Spatial Dependence). The package includes a range of functions that are useful for creating spatial weighting schemes which are useful for identifying the spatial structure of datasets, as well as some spatial analysis methods including the spatial autocorrelation measures we will use here.

To highlight some of these techniques, we will return to the data on vaccination uptake from the previous tutorial. Let's load in the data and remind ourselves of the spatial patterns it contained.

```{r}
# Get data ready
msoas <- read_sf("./Shapefiles/Liverpool_MSOAs/england_msoa_2011.shp")  # Load in shapefile for Liverpool MSOAs
vaccine_uptake <- read.csv("./Data/msoa_vaccine_10June21.csv") # Load in vaccine uptake data
msoas <- merge(msoas, vaccine_uptake, by.x = "code", by.y = "msoa_code", all.x = TRUE) # Merge the vaccine uptake objects onto the msoa object, based on the columns defined (x = msoas, y = vaccine_uptake), and do this for all observations only in the x (msoas) object

# Map
map7 <- ggplot() + # Call ggplot command
            geom_sf(data = msoas, aes(fill = total_first_dose), lwd = 0) + # Using a spatial object, plot MSOAs and fill in based on number of people with first COVID-19 dose, with line width = 0 (i.e., not visible)
            scale_fill_viridis_c(option = "plasma") + # Make colour-blind friendly
            xlab("Longitude") + # Add x-axis label
            ylab("Latitude") + # Add y-axis label
            labs(title = "First dose COVID-19 vaccinations", # Add title to map
                 fill = "Frequency") # Edit legend title
map7 # Print plot

```

### Global Moran's I

The first step we would want to take is to describe the overall extent of spatial clustering of any variable. To do this, we will calculate a global Moran's I statistic that can tell us the extent of clustering of data. You can review the methodology behind this in the related lecture slides [here]().

To be able to describe the clustering, we need to tell R about the spatial structure of our dataset. Specifically, we need to identify which areas are located next to each other (i.e., neighbours).

As we are dealing with area (polygons) spatial data, we will define spatial neighbours based on which borders they match on. So if two areas share a common border, then we can define them as neighbouring each other. There are two ways of defining this process:

* Rook contiguity - only areas that share common borders/edges of some defined length
* Queen contiguity - all areas that touch each other are considered neighbours, even if the shared space is very small

To calculate the spatial structuring of neighbouring areas, we run the following code.

```{r}
library(spdep) # Load package
nb <- poly2nb(msoas, queen = TRUE) # Calculate queen contiguity for areas (set queen = FALSE for rook contiguity)

```

We now need to assign spatial weights to each area (polygon) based on the spatial structure of the data. These weights are important when we are calculating the average values of neighbours, but adjusting their values.

```{r}
lw <- nb2listw(nb, style = "W", zero.policy = TRUE) # Assign weights based on list of neighbours. Each neighbouring area is given an equal weighting (syle = "W"). Zero.policy = TRUE allows for areas with no neighbours.

```

Now that we have weightings for each areas linked to its neighbours, we can calculate the global Moran's I. Essentially, it is looking at the correlation between each area's value and the average values for surrounding neighbours. This gives a crude descriptive value of the amount of clustering in our data - it is termed 'global' since it is an average for all areas.

```{r}
m1 <- moran.test(msoas$total_first_dose, lw) # Calculate Moran's I
m1 # Print result

```

There is a lot of output here, however we are really only interested in two things mostly. The Moran's I statistic is `r m1$estimate[1]`. We interpret the estimated value was running between -1 and 1, where values closer to 1 suggest existence of spatial clustering, values of 0 suggesting no clustering (i.e., random), and values less than 0 suggesting evenly dispersed values (rare in the real world). Some rules of thumbs - a value of 0.3 would suggest low-moderate clustering, with values 0.5+ suggestung moderate to high clustering. A value of `r m1$estimate[1]` suggests little evidence of clustering in the data.

We can also consider the p-value to assess if the result is statistically significant ~~notwithstanding the continued controversy on using p-values~~. A value of `r m1$p.value` suggests that the result is not statistically significant, lending further evidence towards a conclusion that there is little evidence of spatial clustering. 

To get a more accurate p-value, we should use Monte Carlo simulations to estimate it. Let's try this, but this time looking at the clustering of second dose COVID-19 vaccinations.

```{r}
m2 <- moran.mc(msoas$total_second_dose, lw, nsim = 1000) # Calculate Moran's I and simulate 1000 times permutations to give more accurate p-value
m2 # Print result

```

Here we find some evidence of clustering in the data, with a value of `r m2$estimate[1]` suggesting low to moderate clustering in the data.

```{r}
m2_plot <- moran.plot(msoas$total_second_dose, listw = lw, plot = FALSE)  # Save raw moran's I data (if run without saving as an object, it will plot in base R if you set plot = TRUE (default)- for our purposes, I have shown how to get it into ggplot2 as it looks nicer

# Plot
plot1 <- ggplot(data = m2_plot, aes(x = x, y = wx)) + # Plot x (actual values) and wx (average for neighbours)
    geom_point() + # Scatter plot
    geom_smooth(method = "lm") + # Line of best fit
    xlab("Number of second doses") + # Label x-axis
    ylab("Spatially lagged number of second doses")
plot1 # Print plot

```

### Local Moran's I

If we find evidence of clustering overall, then the next step is to describe where this clustering exists. The global Moran's I for number of second COVID-19 vaccine doses suggested some clustering of values, let's explore where the clustering exists and the nature of it. Using a local Moran's I, we can calculate the extent each area belongs to a cluster of high or low values.

```{r}
# Estimate metrics
local_2nd <- localmoran(x = msoas$total_second_dose, listw = lw) # Calculate local Moran's I
# local_2nd <- localmoran_perm((x = msoas$total_second_dose, listw = lw, nsim=499) # In case want to run more simulations to get a more accurate level of significance
local_2nd_map <- cbind(msoas, local_2nd) # Join estimates onto shapefile for MSOAs

# Map
map8 <- ggplot() +
  geom_sf(data = local_2nd_map, aes(fill = Ii)) + # Plot local Moran's I statistic (z score)
  scale_fill_viridis() + # Make colour blind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Clustering of second dose uptake", # Edit plot title 
       fill = "Local Moran's I") # Edit legend title (note must match fill as that is what we are plotting)
map8 # Plot

```

The map tells us how different each area is to it's surrounding areas , with areas with higher z-scores representing areas that are clustered by higher values in the surrounding areas, and lower scores the opposite (surrounded by lower values). The map suggests a cluster in the city centre area, as well as a less distinct one to the South East. To understand how meaningful these spatial patterns are, we can plot the p-values for areas.

```{r}
map9 <- ggplot() +
  geom_sf(data = local_2nd_map, aes(fill = Pr.z...0.)) + # Plot local Moran's I statistic (z score)
  scale_fill_viridis() + # Make colour blind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Clustering of second dose uptake", # Edit plot title 
       fill = "p-value") # Edit legend title (note must match fill as that is what we are plotting)
map9 # Plot

```

What the map is missing is the nature of the clusters. We can say with some confidence that there is a cluster in the city centre for example, but we are not sure what this is a cluster of. We could compare manually to `map7`, however it would be useful to have a single map the brings together all of this information. To help contextualise the clustering analysis, we need to classify the local Moran's I data to describe their patterns.

```{r}
# Data wrangling
quadrant <- vector(mode="numeric", length = nrow(local_2nd)) # Create blank object for storing results
m_2nd <- msoas$total_second_dose - mean(msoas$total_second_dose) # Centers each area around its mean (for variable under investigation)
m_localmi <- local_2nd[,1] - mean(local_2nd[,1]) # Centers areas on the local Moran's I values around the mean
sig <- 0.1 # Define statistical significance threshold (feel free to select more stringent values)

# Populate the blank object with our classification of results
quadrant[m_2nd < 0 & m_localmi < 0] <- 1 # Low-low: Lower than average raw value, lower than average surrounding areas
quadrant[m_2nd < 0 & m_localmi > 0] <- 2 # Low-high: Lower than average raw value, higher than average surrounding areas
quadrant[m_2nd > 0 & m_localmi < 0] <- 3 # High-low: Higher than average raw value, lower than average surrounding areas
quadrant[m_2nd > 0 & m_localmi > 0] <- 4 # High-high: Higher than average raw value, higher than average surrounding areas
quadrant[local_2nd[,5] > sig] <- 0 # Identify non-significant areas
quadrant <- factor(quadrant, levels = c("0", "1", "2", "3", "4")) # Define variable as factor (as distinct categories)
local_2nd_map2 <- cbind(msoas, quadrant) # Join data onto the original shapefile

# Plot
map10 <- ggplot() +
  geom_sf(data = local_2nd_map2, aes(fill = quadrant)) + # Plot values
  scale_fill_manual(values = c("0" = "white", "1" = "blue", "2" = rgb(0,0,1,alpha=0.4), "3" = rgb(1,0,0,alpha=0.4), "4" = "red"),
                    labels = c("Insignificant", "Low-low", "Low-high", "High-low", "High-high"),
                    breaks = c(0, 1, 2, 3, 4), 
                    drop = FALSE) + # Show all values in legend
  labs(title = "Clustering of second dose uptake", # Edit plot title 
    fill = "Clusters") + # Edit legend title (note must match fill as that is what we are plotting)
  xlab("Longitude") + # Add labels
  ylab("Latitude")
map10 # Print

```

This map is a common output in local Moran's I analyses. It combines the raw data, clustering analysis and associated statistical significance into a single plot. We can see that in the city centre, we have a cluster of low vaccination uptake surrounded by areas with higher values. We then have two significant clusters to the East, representing clusters of higher uptake.

### Getis-Ord Gi statistic

An alternative clustering metric is the Getis-Ord Gi statistic. The Gi statistic is presented as a z-score, with positive values representing clusters of high values and negative values representing clusters of low values.

We can calculate the Gi statistic using the following code.

```{r}
localGi <- localG(msoas$total_second_dose, listw = lw) # Calculate Gi statistics for each MSOA (second dose total)

```

As before, the power of the clustering metrics is seen when we map it. Let's visualise the spatial pattern of clustering. 

```{r}
# Wrangle data
Gi_map <- cbind(msoas, data.matrix(localGi)) # Join results onto shapefile
names(Gi_map)[names(Gi_map) == "data.matrix.localGi."] <- "gstat" # Rename column

# Map
map11 <- ggplot() +
  geom_sf(data = Gi_map, aes(fill = gstat)) +
  scale_fill_viridis() + # Make colour blind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Second dose uptake", # Edit plot title 
       fill = "Gi statistic") # Edit legend title (note must match fill as that is what we are plotting)
map11

```

The map produced is both similar and different to the local Moran's I. We see clustering in the city centre (younger populations) and north parts of Liverpool (more deprived communities), with higher vaccinate uptake in the more affluent South and South-East parts of the city.

## Summary

So far, you have learned how to use spatial data in R, visualise and map their patterns, and start to analyse the spatial patterns themselves. Next we will move onto spatial extensions of regression techniques.

<!--chapter:end:02-cluster-analysis.Rmd-->

# Spatial Regression {#spatreg}

So far we have learned how to visualise spatial data and explore if patterns display clustering of high/low values. However, what if we want to understand the predictors of spatial patterns? In this section, we extend regression techniques to incorporate the spatial structure of data. The lecture slides for this practical can be found [here](). A PDF version of the practical can be found [here]().

## Exploring the data

In this tutorial, we will focus on understanding geographical patterns in COVID-19 vaccination uptake. Our analysis will use data collected for Local Authorities Districts (LADs). LADs are large administrative areas that correspond to Local Government areas, typically equivalent to a city, large town or region. 

I have collected data on uptake of COVID-19 vaccines (split by number of first and second doses upto 17th June 2021 from [here](https://www.england.nhs.uk/statistics/statistical-work-areas/covid-19-vaccinations/). We will focus on the percentage of people who have had their first vaccination dose as our outcome variable of interest here. I have also compiled a suite of explanatory and contextual variables to help understand patterns in vaccination uptake. These include:

* **Population** data was gathered to provde the denominator for our outcome variables. These data are for mid-year 2019 (~July) and were the most recent available statistics available at the time. Data were downloaded from [here]( https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationestimatesforukenglandandwalesscotlandandnorthernireland). 
* **Population density** is also calculated using the population estimates based on the ratio of people to the size of the area. We use this variable to account for urban and rural differences in population (as a proxy).
* **Median age** was collected from the above population data, to account for the local age structure of areas since older groups could receive their vaccine at an earlier date. 
* **Ethnicity** is measured in aggregated ethnic groups. This was selected because of evidence that some ethnic groups have been targeted with misinformation that may have put them off getting their vaccine. We use estimate population counts for 2019 from [here](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationcharacteristicsresearchtables). I cleaned the data and converted the population estimates into percentages for the purpose of our analyses. The following aggregated ethnic groups are available: White British, Other White, Black or Black British, Asian or Asian British, Other Ethnicity. For analyses, we will look at all groups other than White British as we hypothesise they may have the highest uptake rates.
* **Deprivation** was measured using the Index of Multiple Deprivation score (2019). The composite index is the most commonly used measure of deprivation used by researchers and policy officials. We include deprivation in our analyses as we hypothesise that uptake will be lower in more deprived areas. The data are openly available [here](https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019).

Let's load the data into R and tidy it up.

```{r message=FALSE, warning=FALSE}
# Load package
library(sf)

# Load and clean spatial data
lad_uk <- read_sf("./Shapefiles/UK_LAD/Local_Authority_Districts_(December_2019)_Boundaries_UK_BFC.shp") # Load shapefile for Local Authority Districts (LADs) for UK (sorry but could not find only England version so need to convert to match data)
lad_uk$country <- substr(lad_uk$lad19cd, 0, 1) # Record first letter of LAD code (denotes country)
lad_eng <- lad_uk[lad_uk$country == "E",] # Subset only English LADs

# Tidy and join on explanatory variables
lad_data <- read.csv("./Data/LAD_vaccine_data.csv") # Load vaccine uptake and demographic data for England
lad_eng <- merge(lad_eng, lad_data, by.x = "lad19cd", by.y = "ltla_code", all.x = TRUE) # Join on both datasets
lad_eng$pop_density <- lad_eng$population / (lad_eng$st_areasha / 1000000) # Calculate population density (st_areashape is measured in metres^2 so need to convert to km^2 by dividing by 1,000,000)
lad_eng$percent_first_dose <- (lad_eng$total_first_dose / lad_eng$population) * 100 # Calculate outcome variable

# Remove objects to save space
rm(lad_uk, lad_data) 

```

The first step is to visualise our outcome variable and examine if there are any spatial patterns. Ideally we would age-standardise our outcome since older age groups were allowed to be vaccinated at early dates, but for simplicity we will stick with the raw percentage of uptake ~~don't have a go at my laziness~~.

```{r message=FALSE, warning=FALSE}
# Load packages
library(ggplot2) 
library(viridis)

# Plot
map1 <- ggplot() + # Call ggplot command
  geom_sf(data = lad_eng, aes(fill = percent_first_dose), lwd = 0) + # Define what to plot
  scale_fill_viridis() + # Make colourblind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "First dose uptake", # Edit plot title
       fill = "Percent (%)") # Edit legend title
map1 # Print plot

```

What are the main spatial patterns that you can observe? There is lower uptake in urban areas, especially London, although many of these areas are small on the map due taking up smaller land mass. Else, there is probably not any other distinct spatial pattern.

## Non-spatial regression

If we wanted to understand why uptake was higher or lower in certain areas, we might use a regression model. If we focus on the standard OLS regression model, we utilise the following equation:

$$
y = X\beta + \epsilon
$$

Here, we predict $y$ as a function of a series of predictor $X$ variables that we adjust their effects based on $\beta$ values and some measure of the error $\epsilon$. We can use an OLS regression model to help us explain patterns in uptake, based on our explanatory variables.

```{r}
model1 <- lm(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng) # Fit a linear regression model for the following equation (outcome ~ explanatory)
summary(model1) # Print model results summary

```

Urgh, what ugly output. I mean it is functional, but not pretty. Good thing we can make the output nicer using various packages in R. I really like `gtsummary` which can clean regression tables up.

```{r message=FALSE, warning=FALSE}
library(gtsummary) # Load package
tbl_model1 <- tbl_regression(model1, label = list(median_age ~ "Median age", Other_White ~ "Other White (%)", Mixed ~ "Mixed (%)", Black ~ "Black (%)", Asian ~ "Asian (%)", Other ~ "Other Ethnicity (%)", mean_imd_score ~ "Deprivation score", pop_density ~ "Population density")) # Make tidy table
tbl_model1 # Print

```

What does the model say?

* Median age was positively associated with the percentage of the population who were vaccinated, with areas that had older populations on average being associated with higher uptake.
* There are mixed associations found for the ethnicity variables - a negative association between the percentage of an area's population that were Black or Black British and uptake (i.e., uptake was lower in areas with a higher share of the population that were Black), a positive association between Mixed ethnicity and uptake, and large uncertainty in estimates for 'Asian', 'Other White' or 'Other Ethnicity' communities.
* Deprivation score was negatively associated with uptake, where as areas became more deprived uptake was lower.
* The effect for population density looks misleading in the cleaned table due to rounding issues, but if we scroll back up to the messier table we can see that as population density increases (i.e., larger more populated urban areas) uptake falls

A few questions for you to think about: Was this the correct statistical model? Were the correct explanatory variables used and what happens if you try others? Does the same associations persist if we look at second dose uptake?

One of the classical assumptions of an OLS regression model is the independence of errors (and to some extent observations as well). Since we have spatial data and areas closer together may be similar than those further apart (i.e., the characteristics and populations of Liverpool and Wirral are more similar than say, Liverpool and Guildford), this assumption may not hold. We can assess if this may be an issue through plotting the residuals (i.e., our error term $\epsilon$) from the regression model and exploring if any spatial patterns exist.

```{r message=FALSE, warning=FALSE}
# Join on 
lad_eng <- cbind(lad_eng, model1$residuals)

# Plot
map2 <- ggplot() + # Call ggplot command
  geom_sf(data = lad_eng, aes(fill = model1.residuals), lwd = 0) + # Define what to plot
  scale_fill_viridis() + # Make colourblind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "First dose uptake model", # Edit plot title
       fill = "Residuals") # Edit legend title
map2 # Print plot


```

If there were no issues here, we might expect to find a random pattern. However, we can see this isn't always the case. A positive residual would suggest that the observed value of an area is greater than what the model would predict based on the coefficients and it's local values for each explanatory variable. There are some clustering of values in the North West, London and other urban areas. Similarly, a negative value suggests lower observed uptake than we might expect/predict from the model. We can see evidence of this in the West and South East of England. 

Our analysis may therefore benefit from having a spatial regression model. 

## Selecting the right spatial model

The first thing we might want to check is the extent that there is spatial clustering of our data. We will start here by checking this for our outcome variable and the regression model residuals. We will follow the same methods that we introduced in the [previous session](#cluster).

We will need to identify the spatial structure of our dataset. We will follow the same previous method of assigning neighbouring areas based on Queen's contiguity. One issue here is that we have two Local Authorities that are islands (Isles of Scilly and Isle of Wight) which do not have any neighbours. To solve this, we could either assign the two islands manually to their nearest 'neighbour' (e.g., Isles of Scilly to Cornwall) or remove them from our analysis. For the basis of teaching you the methods here ~~and because I am too lazy to code it up as it is a faff~~, we will just remove them from the data. We ought to re-run our regression model since we are dropping two observations, but we will not to save time here (we will correct this later so stay tuned).

Let's check the spatial clustering in our outcome variable through calculating the Moran's I.

```{r message=FALSE, warning=FALSE}
library(spdep) # Load package
lad_eng <- lad_eng[lad_eng$lad19cd != "E06000053" & lad_eng$lad19cd != "E06000046",] # Drop Isles of Scilly or Isle of Wight
nb <- poly2nb(lad_eng, queen = TRUE) # Calculate queen contiguity for areas (slow)
lw <- nb2listw(nb, style = "W", zero.policy = TRUE) # Assign weights based on list of neighbours
m1 <- moran.test(lad_eng$percent_first_dose, lw) # Calculate Moran's I
m1 # Print result

```

A Moran's I value of `r m1$estimate[1]` would indicate existence of moderate spatial clustering of first COVID-19 vaccination dose uptake.

Next, we will repeat the analysis for the regression model residuals. Remember this is more important in checking the model assumptions.

```{r}
m2 <- moran.test(lad_eng$model1.residuals, lw) # Calculate Moran's I
m2 # Print result

```

Here, a value of `r m2$estimate[1]` suggests weak clustering. While low, the result is statistically significant suggesting it is important variation that we need to take in account in how we approach our regression analysis.

Do you think that was a bit of a faff to code up? Well, you can do the whole thing in a single line of code thanks to `spdep`'s `lm.morantest` command. Indeed, it can allow us to update our regression model with the dropped observations quickly too. ~~OK I should I have said this earlier to not waste your time, but sorry not sorry as it is useful to show the manual process I hope.~~

```{r}
model1 <- lm(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng) # Re-run regression model
m3 <- lm.morantest(model1, lw) # Run Moran's I analysis of residuals
m3 # Print results

```

Again we get a similar result. 

So we have a problem. How might we address it? It might be that we have left out some unmeasured explanatory covariates that would account for the spatial variation. This might not always be possible. A different approach would be to account for the spatial structure of our underlying dataset. We could do this by adding in a categorical variable representing each area as a fixed effect in the regression model. You can try this by re-running the previous code and adding into the formula `lad19cd` - what are the issues this brings? We could also extend this model to be a multi-level regression model where the area identifier is specified as a random effect (check out R package `lme4` for more here). Neither of these models explicitly accounts for the spatial nature of the data (i.e., the regression model does not know the spatial structure of the data).

The other thing we could do is use a spatial regression model that explicitly accounts for the locations of each data point. Here we tell the regression model that the spatial structure of data points matters for their interpretation (i.e., data points closer to each other are more similar than those further apart). There are *a lot* of different types of spatial regression models. How might we select the correct model? 

Selection of models may be based on which specification we think best describes our data. This is hard to decide! The other approach is that we can utilise model fit statistics to assess which spatial models may improve upon the OLS regression model we previously fit. We can do this using `spdep`'s `lm.LMtests` function. Here we test for different features of **spatial dependence** in our data/model. For review here, please consult the lecture slides for this practical [located here](). Tl;dr spatial dependence is where the spatial configuration (i.e., structure of locations) affects our outcome.

We will focus in this practical on spatial lag and spatial error models. The following code tests for whether a spatially lagged dependent variable or spatial error dependence can improve our model fit. We can test for more things using this code, but for now we stick with these four tests. 

```{r}
spat_dep_test <- lm.LMtests(model1, lw, test=c("LMerr", "LMlag", "RLMerr", "RLMlag")) # Test for spatial dependence
spat_dep_test

```

If we consider all of the tests, we can see that for each test of spatial dependence that they are each statistically significant. This would suggest that each spatial model can benefit our analysis and model fit. If none were significant, then we would use the OLS regression results. 

## Spatial lag model

The first types of spatial models we will consider are those which incorporate a **spatial lag**. These models use variables that are spatially lagged, which means that they calculate measures for each area that characterise (e.g., mean value) their surrounding neighbours. Spatial lags might correspond to the outcome variable or explanatory variables. A spatial lag suggests that the surrounding areas have an influence on the outcome of an area.  

### SLX spatially lagged model

The first spatial regression model we will consider is the **SLX spatial lag model**. SLX here means Spatially Lagged X-variables. We define the model as:

$$
y = X \beta + WX \theta + \epsilon
$$

The equation is a simple extension of the OLS regression equation. Our outcome variable $y$ is a function of our explanatory variables $X$ and their $\beta$ coefficients, a spatial lag coefficient $\theta$ of the $X$ variables based on a spatial weight $W$ and the error term $\epsilon$. The $\beta$ value represents the *direct effect* of an explanatory variable and the $\theta$ value represents the *indirect effect*. An indirect effect is synonymous with a *spillover effect* whereby changes in $x$ in an area have on it's surrounding neighbours based on how $W$ is defined. The spatial lags are exogenous in definition.

To fit the model, we use the R package `spatialreg` which allows us to fit cross-sectional spatial regression models. We re-run the previous analysis of first dose uptake using this spatial model and tidy the output (please note that `gtsummary` does not handle spatial models well).

```{r message=FALSE, warning=FALSE}
library(spatialreg) # Load package
model2 <- lmSLX(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, lw) # Spatial lag model
tbl_model2 <- tbl_regression(model2, label = list(median_age ~ "Median age", Other_White ~ "Other White (%)", Mixed ~ "Mixed (%)", Black ~ "Black (%)", Asian ~ "Asian (%)", Other ~ "Other Ethnicity (%)", mean_imd_score ~ "Deprivation score", pop_density ~ "Population density", lag.median_age ~ "Lag: Median age", lag.Other_White ~ "Lag: Other White (%)", lag.Mixed ~ "Lag: Mixed (%)", lag.Black ~ "Lag: Black (%)", lag.Asian ~ "Lag: Asian (%)", lag.Other ~ "Lag: Other Ethnicity (%)", lag.mean_imd_score ~ "Lag: Deprivation score", lag.pop_density ~ "Lag: Population density")) # Tidy model output
tbl_model2 # Print

```

To interpret the model can be difficult. The $\beta$ coefficients are not exactly the same. Rather, to understand the marginal effect of our covariates, we need to estimate their total impacts (i.e., direct effect + indirect effect). To do this, we use the following piece of code.

```{r}
model2_imp <- impacts(model2, listw = lw) # Estimate direct, indirect and total effects of variables
model2_imp # Print

```

You will see here these are exactly the same as $\beta$ coefficients for this model, however this will not always be the case. What is useful here is that we can see the direct impact in an area, the indirect effects surrounding each and the total effect of each factor considering both together. The spatial lags here are mostly non-statistically significant, other than for population density. It suggests that they bring little to the model.

Standard errors and p-values for these statistics can be estimated through the following modified version of the code.

```{r}
model2_imp_se <- summary(impacts(model2, lw), zstats = TRUE) # Estimate
model2_imp_se # Print

```

We can also compare the results from the spatial model to our original OLS regression model

```{r message=FALSE, warning=FALSE}
tbl_merge(list(tbl_model1, tbl_model2)) # Combine both model outputs together

```

Not a lot has changed if we compare the direct effects of coefficients (many remain within their 95% confidence intervals), probably because the spatial lagged effects were not strong or identifiable. Population density has become non-statistically significant mind you, as has percentage of people who with mixed ethnicity. 

We can also compare model fit to see if the spatial model improves on the original OLS regression model. For example, we can check this by quickly looking at the AIC model fit. 

```{r}
AIC(model1) # OLS regression model
AIC(model2) # SLX spatial lag model

```

The spatial lag model does improve model fit overall. 

### SAR Spatial Lag

The other spatial lag model that we can specify is the following:

$$
y = \rho Wy + X\beta + \epsilon
$$

SAR stands for Spatial AutoRegressive model. Here, we have the standard OLS regression equation $X\beta + \epsilon$, however we also have included a spatial lag component $\rho W$ of our outcome variable $y$. $W$ is once again our spatial weights matrix, with $\rho$ representing the correlation of the spatial lag of $y$ to $y$.

Since the model contains $y$ at both sides of the equation, it violates the exogeneity assumption of the OLS regression model. We therefore need a different approach to estimate the equation. 

Please note: `gtsummary` does not handle spatial models well, so we will have to ~~fudge it~~ persist a bit here. If you can fix it, send your answers on a postcard/carrier pigeon please. Remember, you can always get the raw output of a model by using `summary(model3)`.

```{r message=FALSE, warning=FALSE}
model3 <- lagsarlm(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, lw) # Model
tbl_model3 <- tbl_regression(model3) # Tidy model output
tbl_model3 # Print

```

Let's compare it to our standard OLS regression model. We will go back to the untidy version of the output to match the above

```{r}
tbl_model1 <- tbl_regression(model1, intercept = TRUE) # Redo Table 1 to match above format
tbl_merge(tbls = list(tbl_model1, tbl_model3), # Join output together
          tab_spanner = c("**OLS**", "**Spatial Lag**")) # Rename columns (in bold)

```

Not a lot has changed between the models, suggesting that the spatial patterns captured very little of the patterns observed in our covariates. The most interesting difference is for our population density variable `pop_density`, which has gone from statistically significant in our OLS regression model to insignificant in our spatial lag model. The percentage of people with their ethnicity as other White (`Other_White`) has now become statistically significant as well, with a negative association to the percentage vaccinated. Otherwise, the coefficients have remained similar and within the 95% Confidence Intervals of the OLS estimates.

The summary output for the spatial lag model gives us only the direct impacts for our coefficients, however we may be interested in the direct, indirect and total effects each covariate is having on our outcome variable.

```{r}
impacts(model3, listw = lw) # Estimate direct and indirect effects

```

The table would suggest that the indirect effects of each covariate on the surrounding areas means that focusing on the direct association alone underestimates the total impact of each variable.

To get the standard errors and p-values for our direct, indirect and total effects, we use the following code. Unlike the SLX model, we cannot estimate these directly and therefore must use simulations (MCMC) approaches to estimate them. This only requires a slight modification to the code, in our case we will add in 500 simulations (the more the merrier, but impacts on processing time).

```{r}
summary(impacts(model3, listw = lw, R = 500), zstats = TRUE) # Slow, but gives quantiles etc

```

It gives a lot of output, but it is useful to pick apart how useful our estimates are.

## Spatial error models

We next explore how to implement spatial error models. The underlying models are modified OLS regression models meaning the models are more straightforward ways of accounting for spatial processes in our analysis, as well as being fairly easy to interpret. In this section, we will deal with models that incorporate the spatial structure of the dataset within the error terms. Remember that this was the key OLS assumption that might have been violated (i.e., independence of errors). 

### Spatial Error Model (SEM)

The main equation that we are trying to estimate in our spatial error model is:

$$
y = X\beta + u
$$

Here, we are trying to explain our outcome variable $y$ based upon our the $\beta$ coefficients of covariate variables $X$ plus our error term $u$. The equation itself looks fairly similar to the OLS equation, until we introduce the spatial autocorrelation element to our model. We partition $u$ into the following:

$$
u = \lambda Wu + \epsilon
$$

We define $u$ as a coefficient $\lambda$ controlling the spatial weight $W$ placed on the the error terms $u$, plus the general error term $\epsilon$ for our model.

We cannot use the OLS regression model to estimate the equation, since the assumption of independence of error terms is violated. We therefore require new approaches to do this. Again, `gtsummary` doesn't like these types of models but I have included here as it helps clean the output up.

```{r}
model4 <- errorsarlm(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, lw) # Model
tbl_model4 <- tbl_regression(model4) # Tidy model output
tbl_model4 # Print

```

These models do not have direct and indirect effects since the spatial component only affects the error term. 

We may also want to check whether a spatial error model was relevant here through running a **Hausman Test** which compares if the model improves upon the OLS model.

```{r}
HausmanTest <- Hausman.test(model4) # Run test
HausmanTest # Print

```

The test suggests that the spatial error improves the model (i.e., we want it to be statistically significant).

We can compare the model output to the OLS and spatial lag models too.

```{r message=FALSE, warning=FALSE}
tbl_merge(tbls = list(tbl_model1, tbl_model3, tbl_model4), # Join three model outputs together
          tab_spanner = c("**OLS**", "**Spatial Lag**", "**Spatial Error**")) # Rename columns (in bold)

```

The spatial error model looks relatively similar to the spatial lag model, although there are some differences with `Mixed` and `Asian` variables now being statistically insignificant. 

### Spatial Durbin Error

We can extend the spatial error model through adding in spatial lags for our x-variables, accounting for the spatial effects of independent variables. This requires only a minor modification of the spatial error model equation:

$$
y = X\beta + WX\theta + u
$$
The key difference to the spatial error model is the introduction of $WX\theta$ which applies the spatial weight $W$ to the independent variables $X$ and our spatial autocorrelation coefficient $\theta$.

$u$ again is defined as:

$$
u = \lambda Wu + \epsilon
$$

We can fit this model using a slight change to the spatial error code. Please note that `gtsummary` does not like this type of model output either, so it is a little messy again. 

```{r message=FALSE, warning=FALSE}
model5 <- errorsarlm(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, lw, etype = "emixed") # Fit model
tbl_model5 <- tbl_regression(model5) # Tidy model output
tbl_model5 # Print

```

The inclusion of spatial lags does not seem to bring much advantage here. Since we have introduced spatial lags into our model, we can also calculate the direct, indirect and total effects of covariates too. I have included the code for calculating standard errors, however we don't need to run it again ~~because I am lazy~~.

```{r}
impacts(model5, listw = lw) # Calculate indirect, direct and total effects
# summary(impacts(model3, listw = lw, R = 500), zstats = TRUE) # Slow, but gives quantiles, standard errors and p-values

```

There are more extensions of these models ~~than you can shake a stick at~~ we could explore here - for example, SARAR or SARMA - and most of them are essentially different combinations of spatial lags and spatial error models. We can save them for a rainy day.

## Summary

In this practical session, we have explored how to extend the standard OLS regression model to incorporate spatial processes. We will continue this theme in the next, and final, session where we examine Geographically Weighted Regression.


<!--chapter:end:03-spatial-regression.Rmd-->

# Geographically Weighted Regression {#gwr}

In our last session (boo), we extend the spatial regression approach to explore the concept of spatially varying coefficients. The lecture slides for this practical can be found [here](). A PDF version of the practical can be found [here]().

## Loading data

We will use the same data introduced in the [previous session]({#spatreg}). We will this time look at second dose uptake to mix it up. Let's quickly load in the data and tidy it.

```{r}
# Load package
library(sf)

# Load and clean spatial data
lad_uk <- read_sf("./Shapefiles/UK_LAD/Local_Authority_Districts_(December_2019)_Boundaries_UK_BFC.shp") # Load shapefile for Local Authority Districts (LADs) for UK (sorry but could not find only England version so need to convert to match data)
lad_uk$country <- substr(lad_uk$lad19cd, 0, 1) # Record first letter of LAD code (denotes country)
lad_eng <- lad_uk[lad_uk$country == "E",] # Subset only English LADs

# Tidy and join on explanatory variables
lad_data <- read.csv("./Data/LAD_vaccine_data.csv") # Load vaccine uptake and demographic data for England
lad_eng <- merge(lad_eng, lad_data, by.x = "lad19cd", by.y = "ltla_code", all.x = TRUE) # Join on both datasets
lad_eng$pop_density <- lad_eng$population / (lad_eng$st_areasha / 1000000) # Calculate population density (st_areashape is measured in metres^2 so need to convert to km^2 by dividing by 1,000,000)
lad_eng$percent_first_dose <- (lad_eng$total_first_dose / lad_eng$population) * 100 # Calculate percent of population who have had their first dose
lad_eng$percent_second_dose <- (lad_eng$total_second_dose / lad_eng$population) * 100 # Calculate percent of population who have had their first dose

# Remove objects to save space
rm(lad_uk, lad_data) 

```

Let's have a look at the spatial pattern of the variable. 

```{r}
# Load packages
library(ggplot2) 
library(viridis)

# Plot
map1 <- ggplot() + # Call ggplot command
  geom_sf(data = lad_eng, aes(fill = percent_second_dose), lwd = 0) + # Define what to plot
  scale_fill_viridis() + # Make colourblind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Second dose uptake", # Edit plot title
       fill = "Percent (%)") # Edit legend title
map1 # Print plot

```

We can see lower uptake in urban areas and higher uptake in rural regions. This is likely reflecting differences in the age-structure of areas.

Let's use a standard linear regression model to examine the factors that are associated with the percentage of individuals who are fully vaccinated (i.e., received their second vaccine dose). This will be useful to compare to the **Geographically Weighted Regression (GWR)** model later. 

```{r}
model1 <- lm(percent_second_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng) # OLS model
summary(model1) # Print model results

```

The model results suggest the following associations:

* Median age of a Local Authority is positively associated with the percentage of the population who have had their second vaccine dose, so that older populations had more fully vaccinated individuals
* Greater percentages of areas with Black or Other White ethnic groups had lower uptake, with a negative association detected.
* Population density was negatively associated to vaccination uptake, with uptake decreasing with increasing population density.

## Selecting bandwidths

GWR is a technique that allows us to examine how associations between variables may vary across space. It works through selecting a 'search window' (here defined as surrounding areas) over each data point (Local Authority District in our example), estimating a regression equation (for the data point and data points in the search window, with closer neighbouring data points given larger weightings), and then repeating the process for all data points. This means the process estimates *n* regression equations each time (you can probably see here how it can become computationally intensive). The result is a series of regression coefficients for each variable and each area, allowing us to explore how coefficients vary across space.

To be able to estimate a model, we need to define the 'search window'. This consists of two components: (i) a spatial kernel, and (ii) bandwidth. The **spatial kernel** refers to the weighting mechanism that gives greater importance/weighting to data points located closer to each data point (and vice versa), and the extent that the weighting changes with distance. The kernel can be fixed (i.e., the same bandwidth, such as a fixed distance, is used for each regression) or adaptive (i.e., varying bandwidths are used, such as nearest number of neighbours). **Bandwidith** is the extent of the kernel (i.e., how big an area it covers). 

We can optimise the bandwidth value using a cross-validation. Here we estimate a regression model for a particular location with a set bandwidth value. We then compare the predicted outcome value for the model to the observed/actual value, which gives us the residual error. We can then vary the bandwidth value and see how the residual changes, with the aim of minimising it. 

```{r}
library(spgwr) # Load package
fixed_bandwidth <- gwr.sel(percent_second_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, coords = cbind(lad_eng$long, lad_eng$lat), adapt = FALSE, method = "cv", longlat = TRUE) # Select best fixed bandwidth for GWR (can be slow)
fixed_bandwidth

```

The output prints details of the model fitting process, but you can switch this off by adding `verbose = FALSE`. The optimised bandwidth for our model is `r fixed_bandwidth` kilometers (km). The value suggests that a fixed radius of this distance is set and placed around each data point (area). 

Let's repeat the process, but this time estimate an adaptive bandwidth. This will be useful for comparing model fit later.

```{r}
adaptive_bandwidth <- gwr.sel(percent_second_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, coords = cbind(lad_eng$long, lad_eng$lat), adapt = TRUE, method = "cv", longlat = TRUE, verbose = TRUE) # Select best adaptive bandwidth for GWR (can be slow)
adaptive_bandwidth

```

Here the value of `r adaptive_bandwidth` presents the optimal proportion of neighbours (or k-nearest neighbours) to select as the bandwidth. In this example, we should select `r adaptive_bandwidth * 100`% of areas surrounding each data point (or nearest neighbours), or equivalent to selecting `nrow(lad_eng) * adaptive_bandwidth` areas around each data point each time we run a regression.

## Running the model

Now that we are ready to fit our GWR model, there are two key areas we need to concentrate on when interpreting any GWR model: (i) model fit, and (ii) the meaning of spatially varying coefficients.

### Assessing model fit

We have so far two types of bandwidth to use in fitting our GWR model. We need to identify which we will use for reporting our results. To make a decision, we will fit two GWR models, one for each of the two bandwidths, and compare their model fits to see which performs better.

```{r}
# Model with fixed bandwidth
model2_fixed <- gwr(percent_second_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, coords = cbind(lad_eng$long, lad_eng$lat), bandwidth = fixed_bandwidth, hatmatrix = TRUE, se.fit = TRUE, longlat = TRUE) 

# Model with adaptive bandwidth
model2_adapt <- gwr(percent_second_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, coords = cbind(lad_eng$long, lad_eng$lat), adapt = adaptive_bandwidth, hatmatrix = TRUE, se.fit = TRUE, longlat = TRUE) # note we use adapt for the bandwidth here

```

Let's compare the overall model fit for both of these models. We will just look at the corrected AIC values

```{r}
model2_fixed$results$AICh # AIC - fixed bandwidth
model2_adapt$results$AICh # AIC - adaptive bandwidth

# model2_fixed$results$AICb # AIC corrected for small sample sizes - fixed bandwidth 
# model2_adapt$results$AICb # AIC corrected for small sample sizes - adaptive bandwidth

```

The adaptive bandwidth model has better fit (lower value) and therefore may be preferable on this statistic.

We will next compare the compare the range of local r2 values, to assess model fit performance of our regression models..

```{r}
summary(model2_fixed$SDF$localR2)
summary(model2_adapt$SDF$localR2)

```

Mean and median R2 is higher with the adaptive bandwidth selected, suggesting that on average each areas model fit is better here. If we look at the minimum values, model fit is poorer for the fixed bandwidth as well suggesting the model does less well in particular areas.

Next, we compare the local model fit values to see if one of the models is under/over-performing in particular parts of England. This can give us clues towards whether there are geographical issues in model fit. For example, the use of fixed bandwidths can often lead to too many dissimilar data points selected in regression models (leading to poorer fitting models), or too few data points leading to large uncertainty in estimates. Adaptive bandwidths may vary in their performance spatially given their different sizes of bandwidths.

First, we plot the local R^2 values for the fixed bandwidth model.

```{r}
# Tidy data
results_fixed <- as.data.frame(model2_fixed$SDF) # Save coefficients
lad_eng$fixed_r2 <- results_fixed$localR2 # Add local r2 value to data for mapping

# Plot
map2 <- ggplot() + # Call ggplot command
  geom_sf(data = lad_eng, aes(fill = fixed_r2), lwd = 0) + # Define what to plot
  scale_fill_viridis(limits = c(0, 1)) + # Make colourblind friendly (and set limits to plot for consistency)
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Local model fit (fixed bandwidth)", # Edit plot title
       fill = "R2 value") # Edit legend title
map2 # Print plot

```

Model fit looks good, but with poorer fit in the North West of England (e.g., Cumbria). Let's repeat this for the adaptive bandwidth model.

```{r}
# Tidy data
results_adapt <- as.data.frame(model2_adapt$SDF) # Save coefficients
lad_eng$adapt_r2 <- results_adapt$localR2 # Add local r2 value to data for mapping

# Plot
map3 <- ggplot() + # Call ggplot command
  geom_sf(data = lad_eng, aes(fill = adapt_r2), lwd = 0) + # Define what to plot
  scale_fill_viridis(limits = c(0, 1)) + # Make colourblind friendly (and set limits to plot for consistency)
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Local model fit (adaptive bandwidth)", # Edit plot title
       fill = "R2 value") # Edit legend title
map3 # Print plot

```

There is some poorer fit in the Northern England and in Cornwall, but otherwise it looks fairly good. 

In sum, both of the models are very good. For the purposes of this tutorial, we will use the adaptive bandwidth since it generally has better model fit. 

### Plotting coefficients

Let's begin through looking at our overall summary of our analytical model. We can print out the raw output just by running the object in R. I don't know of a way of cleaning this into a nice and tidy table ~~sorry, not sorry~~. 

```{r}
model2_fixed

```

There is a lot of output and information here. Let's just focus on the summary of the GWR coefficients, which is typically what would be reported in a report. The table presents summary statistics for the coefficients in the model (each coefficient is a row) across all of the local regressions (in our case, all 317 regression models). We may be interested in the minimum and maximum values to see what the range of values are (which is a useful first step to see if coefficients vary in direction). The global model is the same as OLS coefficients and is a useful point of reference to compare to estimates generated in the GWR model. The 1st Quartile, Median and 3rd Quartile values are also useful for considering the variation in values in terms of direction of association and magnitude of strength. 

The next step will be to visualise the spatial variations in coefficient values. This will allow us to see if there are any distinct geographical patterns in relationships. We will just plot for the variable IMD score (deprivation). First, let's plot the coefficient values.

```{r}
# Get data
lad_eng$imd_coef <- model2_fixed$SDF$mean_imd_score # Coefficients

# Plot
map4 <- ggplot() + # Call ggplot command
  geom_sf(data = lad_eng, aes(fill = imd_coef), lwd = 0) + # Define what to plot
  scale_fill_viridis() + # Make colourblind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Deprivation", # Edit plot title
       fill = "Coefficient") # Edit legend title
map4 # Print plot

```

We may also want to plot the standard errors to look at the variability in the precision of our coefficient estimates.

```{r}
# Get data
lad_eng$imd_coef_se <- model2_fixed$SDF$mean_imd_score_se # Standard error

# Plot
map5 <- ggplot() + # Call ggplot command
  geom_sf(data = lad_eng, aes(fill = imd_coef_se), lwd = 0) + # Define what to plot
  scale_fill_viridis() + # Make colourblind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Deprivation", # Edit plot title
       fill = "Standard Error") # Edit legend title
map5 # Print plot

```

The next step would be to assess the statistical significance of the coefficients to identify if any associations were meaningful. To do this, we estimate the t-value and then categorise observations if they meet 95% level of significance. Let's plot areas based on whether they meet the criterion.

```{r}
# Calculate t statistic
lad_eng$t_imd_coef = results_fixed$mean_imd_score / results_fixed$mean_imd_score_se

# Categorise t values as significant or not
lad_eng$t_imd_coef_cat <- cut(lad_eng$t_imd_coef,
                     breaks=c(min(lad_eng$t_imd_coef), -2, 2, max(lad_eng$t_imd_coef)),
                     labels=c("Sig.","Non-sig.", "Sig."))

# Plot
map6 <- ggplot() + # Call ggplot command
  geom_sf(data = lad_eng, aes(fill = t_imd_coef_cat), lwd = 0) + # Define what to plot
  scale_fill_viridis_d() + # Make colourblind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "Deprivation", # Edit plot title
       fill = "Statistical significance") # Edit legend title
map6 # Print plot

```

It may be useful to combine the coefficient and statistical significance plots into one single visualisation. You could do this by joining the two plots together side-by-side using a R package like `patchwork`. Rather, we will only plot significant associations and hide those which are not.

```{r}
# Plot
map7 <- ggplot() + # Call ggplot command
    geom_sf(data = lad_eng, lwd = 0) + # Plot all areas as base layer
    geom_sf(data = lad_eng[lad_eng$t_imd_coef_cat == "Sig.",], aes(fill = imd_coef), lwd = 0) + # Plot the coefficients that are significant
    scale_fill_viridis() + # Make colourblind friendly
    xlab("Longitude") + # Add x-axis label
    ylab("Latitude") + # Add y-axis label
    labs(title = "Deprivation", # Edit plot title
         fill = "Coefficient") # Edit legend title
map7 # Print plot

```

Why not have a look at other coefficients now - what can you find out? What interesting spatial patterns are there?

## Scaling GWR for large datasets

GWR models are computationally intensive to fit and therefore do not scale well with larger datasets (even those with 10000+ observations can be demanding). Some clever cookies have adapted the methodology to estimate the model quicker when dealing with larger or more complex datasets, including creating the R package `scgwr` (SCalable GWR) to implement the method in R. You can read more about the methodology [here](https://www.tandfonline.com/doi/full/10.1080/24694452.2020.1774350).

The actual code is not too difficult to use, but may require some refinement to model parameters. Specifically, we may want to play about with:

* `knn` - Number of nearest-neighbours selected during estimation: larger is better for bigger datasets.
* `nsamp` - Number of random samples for (aproximate) cross-validation: should be smaller than sample size, but larger is better for minimising errors due to random sampling. 

Let's re-run the model again, but using the `scgwr` package. Unfortunately, this code does not work with our example so I leave it in here to show the option for you in case you ever need it. If you can fix it, please send in your answers on a postcode.

```{r}
# # Library
# library(scgwr)
# 
# # Tidy data
# outcome <- lad_eng[, "percent_second_dose"] # Store outcome variable (if don't define as object then stores as list)
# xvars <- lad_eng[, c("median_age", "Other_White", "Mixed", "Black", "Asian", "Other", "mean_imd_score", "pop_density")] # Store explanatory variables
# xy <- lad_eng[, c("long", "lat")] # Store co-ordinates
# 
# # Model
# model3 <- scgwr(y = outcome, x = xvars, coords = xy, knn = 100, kernel = "gau", p = 4, approach = "CV") # Model using cross-validation approach ("CV") and Gaussian kernel ("gau")

```

The R package can incorporate parallel processing for faster processing as well using the `scgwr_p` command. 

## Summary

In this practical session, we have explored how use Geographically Weighted Regression and explored the opportunities that it can bring for exploring spatially varying contexts.

<!--chapter:end:04-GWR.Rmd-->

# Summary {#summary}

Well done on making it to the end! Before we sign off, it is a good time to reflect over everything you have achieved over the past few weeks.

## Learning outcomes

Let's review how you have achieved each of the learning outcomes for this section of the module.

1. Produce static and interactive visualisations of spatial data.

In the [first session](#intro), you learned how to load spatial data into R and map the data using a variety of packages. You produced different types of maps, including how to present them effectively or make maps interactive.

2. Identify clustering of point- and area-based data.

Our [second session](#cluster) looked at how we might try to identify spatial clusters for data. First, descriptive techniques were applied on point-based data. Second, using area-based data you calculated spatial weights and looked for clusters using Moran's I approaches.

3. Extend regression-based approaches to incorporate spatial context.

The final learning outcome was achieved through two sessions. First, we extended OLS regression approaches using spatially lagged variables or accounted for the spatial structure of error terms to accommodate [spatial regression techniques](#spatreg). Second, we extended OLS regression approaches to incorporate spatially varying coefficients through [Geographically Weighted Regression](#gwr).

## Further learning

The methods we have covered so far will have given you a good grounding in spatial methods, however they are just a small flavour of the vast range of opportunities thinking spatially can bring to research. Here are a few other areas or methods you can read up on in case you want to take things forward. For each, I have provided a (open access) research example of its application and some example R code/packages for you to see how it can be done.

* **Cartograms** -> *Description:* Maps can lie. They can distort patterns, resulting in the misleading interpretation of data. One distortion comes from the geographical size of zones. Where zones are small they can be hard to see, and hence larger areas may attract your attention. You may have noticed this in the [spatial regression session](#spatreg) when some cities were difficult to see on the maps. Cartograms (including linked methods for distorting area sizes such as hexmaps) offer one way of minimising this bias, through readjusting the geographical sizes of areas based on their underlying population sizes. This can allow for fairer comparisons, especially when mapping *people* rather than *places*. *Research example:* [Worldmapper: The Human Anatomy of a Small Planet](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0040001). *R example:* [Tutorial using ggplot2](https://r-charts.com/spatial/cartogram-ggplot2/).
* **Spatial panel model** -> *Description:* The spatial regression models we have introduced are cross-sectional (i.e., a single point in time) and can not account for the longitudinal nature of datasets (i.e., where we have repeated data points over time for each area). These models can be extended longitudinally, similar to how we might extend the classical regression model to incorporate time, to give stronger tests of associations between predictors and outcomes. *Research example:* [Determining the spatial effects of COVID-19 using the spatial panel data model](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7139267/). *R example:* [Package spml](http://www.jstatsoft.org/v47/i01/).
* **Spatial multi-level** -> *Description:* Multi-level modelling revolutionised health geography, since through nesting individuals within areas you could control for individual level characteristics and, in theory, separate out differences which difference between areas can explain. While these methods are powerful, they do not explicitly account for space since the model does not which areas are located where. There exists spatial extensions to multi-level models. *Research example:* [Methodological paper extending multi-level models to incoporate spatial effects](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130761). *R example:* [HSAR package](https://cran.r-project.org/web/packages/HSAR/HSAR.pdf).
* **Spatial interaction models and flow data** -> *Description:* Flow data are not frequently found in spatial health research. They refer to where we have observations of spatial interactions between two places. The most common application might be movement data (e.g., in- and out-migration flows between two places). Spatial interaction models extend these data and organise them into a regression framework, that can model the interactions between places to understand what may drive them. *Research example:* [Using a Spatial Interaction Model to Assess the Accessibility of District Parks in Hong Kong](https://www.mdpi.com/2071-1050/9/11/1924). *R example:* [Spatial Interaction Models for Dummies](https://rpubs.com/adam_dennett/257231).
* **Location-allocation** -> *Description:* Where we have spatial points representing sites/locations (e.g., health services), we can try to geographically optimise the locations of sites to improve geographical coverage. This can be useful for locating new site locations, including finding where it is best to place new sites to maximise coverage in areas with low access. *Research example:* [Evaluating the locations of asymptomatic COVID-19 test sites in Liverpool](https://extra.shu.ac.uk/ppp-online/wp-content/uploads/2021/04/thinking-spatially-roll-out-testing-liverpool.pdf). *R example:* [Replicatable code from research example](https://github.com/markagreen/mapping_test_accessibility/blob/main/scripts/location_allocation_model.R).
* **Bayesian extensions** -> *Description:* Bayesian models offer an alternative approach to analysis than frequentist methods. There are a lot of different spatial extensions to Bayesian models, allowing for more flexibility to how we approach our analyses. These approaches are important if we want to utilise generalised linear models. *Research example:* [Evaluating social and spatial inequalities in COVID-19 testing in Liverpool](https://www.sciencedirect.com/science/article/pii/S2666776221000843). *R example:* [Geospatial Health Data Book](https://www.paulamoraga.com/book-geospatial/).
* **Causal inference methods** -> *Description:* There are a large number of techniques that have tried to implement causal inference approaches within a spatial framework. One example of these methods might be spatial regression discontinuity approaches, where you assess impacts of interventions in places through comparisons to the impacts closest to them where they were not implemented. *Research example:* [Review of spatial causal inference methods](https://arxiv.org/abs/2007.02714). *R example:* [SpatialRDD package](https://github.com/axlehner/SpatialRDD).
* **Spatial machine learning** -> *Description:* A relatively newer area of research explores how we can extend machine learning methods to explicitly incorporate space into them. This field is so new that I have very little to report here! *Research example:* [Spatio-temporal predictions using deep learning](https://www.nature.com/articles/s41598-020-79148-7). *R example:* [Statistical Learning tutorial](https://geocompr.robinlovelace.net/spatial-cv.html).

## Thank you

I would just like to end by thanking you for taking the time to read through these materials and hopefully you have enjoyed learning about spatial data analysis. If you have any further questions, please do not hesitate to get in contact.

Dr Mark A. Green \
Senior Lecturer in Health Geography \
University of Liverpool \
<mark.green@liverpool.ac.uk>

<!--chapter:end:05-summary.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

