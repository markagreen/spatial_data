# Spatial Regression {#spatreg}

So far we have learned how to visualise spatial data and explore if patterns display clustering of high/low values. However, what if we want to understand the predictors of spatial patterns? In this section, we extend regression techniques to incorporate the spatial structure of data. The lecture slides for this practical can be found [here](). A PDF version of the practical can be found [here]().

## Exploring the data

In this tutorial, we will focus on understanding geographical patterns in COVID-19 vaccination uptake. Our analysis will use data collected for Local Authorities Districts (LADs). LADs are large administrative areas that correspond to Local Government areas, typically equivalent to a city, large town or region. 

I have collected data on uptake of COVID-19 vaccines (split by number of first and second doses upto 17th June 2021 from [here](https://www.england.nhs.uk/statistics/statistical-work-areas/covid-19-vaccinations/). We will focus on the percentage of people who have had their first vaccination dose as our outcome variable of interest here. I have also compiled a suite of explanatory and contextual variables to help understand patterns in vaccination uptake. These include:

* **Population** data was gathered to provde the denominator for our outcome variables. These data are for mid-year 2019 (~July) and were the most recent available statistics available at the time. Data were downloaded from [here]( https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationestimatesforukenglandandwalesscotlandandnorthernireland). 
* **Population density** is also calculated using the population estimates based on the ratio of people to the size of the area. We use this variable to account for urban and rural differences in population (as a proxy).
* **Median age** was collected from the above population data, to account for the local age structure of areas since older groups could receive their vaccine at an earlier date. 
* **Ethnicity** is measured in aggregated ethnic groups. This was selected because of evidence that some ethnic groups have been targeted with misinformation that may have put them off getting their vaccine. We use estimate population counts for 2019 from [here](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationcharacteristicsresearchtables). I cleaned the data and converted the population estimates into percentages for the purpose of our analyses. The following aggregated ethnic groups are available: White British, Other White, Black or Black British, Asian or Asian British, Other Ethnicity. For analyses, we will look at all groups other than White British as we hypothesise they may have the highest uptake rates.
* **Deprivation** was measured using the Index of Multiple Deprivation score (2019). The composite index is the most commonly used measure of deprivation used by researchers and policy officials. We include deprivation in our analyses as we hypothesise that uptake will be lower in more deprived areas. The data are openly available [here](https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019).

Let's load the data into R and tidy it up.

```{r message=FALSE, warning=FALSE}
# Load package
library(sf)

# Load and clean spatial data
lad_uk <- read_sf("./Shapefiles/UK_LAD/Local_Authority_Districts_(December_2019)_Boundaries_UK_BFC.shp") # Load shapefile for Local Authority Districts (LADs) for UK (sorry but could not find only England version so need to convert to match data)
lad_uk$country <- substr(lad_uk$lad19cd, 0, 1) # Record first letter of LAD code (denotes country)
lad_eng <- lad_uk[lad_uk$country == "E",] # Subset only English LADs

# Tidy and join on explanatory variables
lad_data <- read.csv("./Data/LAD_vaccine_data.csv") # Load vaccine uptake and demographic data for England
lad_eng <- merge(lad_eng, lad_data, by.x = "lad19cd", by.y = "ltla_code", all.x = TRUE) # Join on both datasets
lad_eng$pop_density <- lad_eng$population / (lad_eng$st_areasha / 1000000) # Calculate population density (st_areashape is measured in metres^2 so need to convert to km^2 by dividing by 1,000,000)
lad_eng$percent_first_dose <- (lad_eng$total_first_dose / lad_eng$population) * 100 # Calculate outcome variable

# Remove objects to save space
rm(lad_uk, lad_data) 

```

The first step is to visualise our outcome variable and examine if there are any spatial patterns. Ideally we would age-standardise our outcome since older age groups were allowed to be vaccinated at early dates, but for simplicity we will stick with the raw percentage of uptake ~~don't have a go at my laziness~~.

```{r message=FALSE, warning=FALSE}
# Load packages
library(ggplot2) 
library(viridis)

# Plot
map1 <- ggplot() + # Call ggplot command
  geom_sf(data = lad_eng, aes(fill = percent_first_dose), lwd = 0) + # Define what to plot
  scale_fill_viridis() + # Make colourblind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "First dose uptake", # Edit plot title
       fill = "Percent (%)") # Edit legend title
map1 # Print plot

```

What are the main spatial patterns that you can observe? There is lower uptake in urban areas, especially London, although many of these areas are small on the map due taking up smaller land mass. Else, there is probably not any other distinct spatial pattern.

## Non-spatial regression

If we wanted to understand why uptake was higher or lower in certain areas, we might use a regression model. If we focus on the standard OLS regression model, we utilise the following equation:

$$
y = X\beta + \epsilon
$$

Here, we predict $y$ as a function of a series of predictor $X$ variables that we adjust their effects based on $\beta$ values and some measure of the error $\epsilon$. We can use an OLS regression model to help us explain patterns in uptake, based on our explanatory variables.

```{r}
model1 <- lm(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng) # Fit a linear regression model for the following equation (outcome ~ explanatory)
summary(model1) # Print model results summary

```

Urgh, what ugly output. I mean it is functional, but not pretty. Good thing we can make the output nicer using various packages in R. I really like `gtsummary` which can clean regression tables up.

```{r message=FALSE, warning=FALSE}
library(gtsummary) # Load package
tbl_model1 <- tbl_regression(model1, label = list(median_age ~ "Median age", Other_White ~ "Other White (%)", Mixed ~ "Mixed (%)", Black ~ "Black (%)", Asian ~ "Asian (%)", Other ~ "Other Ethnicity (%)", mean_imd_score ~ "Deprivation score", pop_density ~ "Population density")) # Make tidy table
tbl_model1 # Print

```

What does the model say?

* Median age was positively associated with the percentage of the population who were vaccinated, with areas that had older populations on average being associated with higher uptake.
* There are mixed associations found for the ethnicity variables - a negative association between the percentage of an area's population that were Black or Black British and uptake (i.e., uptake was lower in areas with a higher share of the population that were Black), a positive association between Mixed ethnicity and uptake, and large uncertainty in estimates for 'Asian', 'Other White' or 'Other Ethnicity' communities.
* Deprivation score was negatively associated with uptake, where as areas became more deprived uptake was lower.
* The effect for population density looks misleading in the cleaned table due to rounding issues, but if we scroll back up to the messier table we can see that as population density increases (i.e., larger more populated urban areas) uptake falls

A few questions for you to think about: Was this the correct statistical model? Were the correct explanatory variables used and what happens if you try others? Does the same associations persist if we look at second dose uptake?

One of the classical assumptions of an OLS regression model is the independence of errors (and to some extent observations as well). Since we have spatial data and areas closer together may be similar than those further apart (i.e., the characteristics and populations of Liverpool and Wirral are more similar than say, Liverpool and Guildford), this assumption may not hold. We can assess if this may be an issue through plotting the residuals (i.e., our error term $\epsilon$) from the regression model and exploring if any spatial patterns exist.

```{r message=FALSE, warning=FALSE}
# Join on 
lad_eng <- cbind(lad_eng, model1$residuals)

# Plot
map2 <- ggplot() + # Call ggplot command
  geom_sf(data = lad_eng, aes(fill = model1.residuals), lwd = 0) + # Define what to plot
  scale_fill_viridis() + # Make colourblind friendly
  xlab("Longitude") + # Add x-axis label
  ylab("Latitude") + # Add y-axis label
  labs(title = "First dose uptake model", # Edit plot title
       fill = "Residuals") # Edit legend title
map2 # Print plot


```

If there were no issues here, we might expect to find a random pattern. However, we can see this isn't always the case. A positive residual would suggest that the observed value of an area is greater than what the model would predict based on the coefficients and it's local values for each explanatory variable. There are some clustering of values in the North West, London and other urban areas. Similarly, a negative value suggests lower observed uptake than we might expect/predict from the model. We can see evidence of this in the West and South East of England. 

Our analysis may therefore benefit from having a spatial regression model. 

## Selecting the right spatial model

The first thing we might want to check is the extent that there is spatial clustering of our data. We will start here by checking this for our outcome variable and the regression model residuals. We will follow the same methods that we introduced in the [previous session](#cluster).

We will need to identify the spatial structure of our dataset. We will follow the same previous method of assigning neighbouring areas based on Queen's contiguity. One issue here is that we have two Local Authorities that are islands (Isles of Scilly and Isle of Wight) which do not have any neighbours. To solve this, we could either assign the two islands manually to their nearest 'neighbour' (e.g., Isles of Scilly to Cornwall) or remove them from our analysis. For the basis of teaching you the methods here ~~and because I am too lazy to code it up as it is a faff~~, we will just remove them from the data. We ought to re-run our regression model since we are dropping two observations, but we will not to save time here (we will correct this later so stay tuned).

Let's check the spatial clustering in our outcome variable through calculating the Moran's I.

```{r message=FALSE, warning=FALSE}
library(spdep) # Load package
lad_eng <- lad_eng[lad_eng$lad19cd != "E06000053" & lad_eng$lad19cd != "E06000046",] # Drop Isles of Scilly or Isle of Wight
nb <- poly2nb(lad_eng, queen = TRUE) # Calculate queen contiguity for areas (slow)
lw <- nb2listw(nb, style = "W", zero.policy = TRUE) # Assign weights based on list of neighbours
m1 <- moran.test(lad_eng$percent_first_dose, lw) # Calculate Moran's I
m1 # Print result

```

A Moran's I value of `r m1$estimate[1]` would indicate existence of moderate spatial clustering of first COVID-19 vaccination dose uptake.

Next, we will repeat the analysis for the regression model residuals. Remember this is more important in checking the model assumptions.

```{r}
m2 <- moran.test(lad_eng$model1.residuals, lw) # Calculate Moran's I
m2 # Print result

```

Here, a value of `r m2$estimate[1]` suggests weak clustering. While low, the result is statistically significant suggesting it is important variation that we need to take in account in how we approach our regression analysis.

Do you think that was a bit of a faff to code up? Well, you can do the whole thing in a single line of code thanks to `spdep`'s `lm.morantest` command. Indeed, it can allow us to update our regression model with the dropped observations quickly too. ~~OK I should I have said this earlier to not waste your time, but sorry not sorry as it is useful to show the manual process I hope.~~

```{r}
model1 <- lm(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng) # Re-run regression model
m3 <- lm.morantest(model1, lw) # Run Moran's I analysis of residuals
m3 # Print results

```

Again we get a similar result. 

So we have a problem. How might we address it? It might be that we have left out some unmeasured explanatory covariates that would account for the spatial variation. This might not always be possible. A different approach would be to account for the spatial structure of our underlying dataset. We could do this by adding in a categorical variable representing each area as a fixed effect in the regression model. You can try this by re-running the previous code and adding into the formula `lad19cd` - what are the issues this brings? We could also extend this model to be a multi-level regression model where the area identifier is specified as a random effect (check out R package `lme4` for more here). Neither of these models explicitly accounts for the spatial nature of the data (i.e., the regression model does not know the spatial structure of the data).

The other thing we could do is use a spatial regression model that explicitly accounts for the locations of each data point. Here we tell the regression model that the spatial structure of data points matters for their interpretation (i.e., data points closer to each other are more similar than those further apart). There are *a lot* of different types of spatial regression models. How might we select the correct model? 

Selection of models may be based on which specification we think best describes our data. This is hard to decide! The other approach is that we can utilise model fit statistics to assess which spatial models may improve upon the OLS regression model we previously fit. We can do this using `spdep`'s `lm.LMtests` function. Here we test for different features of **spatial dependence** in our data/model. For review here, please consult the lecture slides for this practical [located here](). Tl;dr spatial dependence is where the spatial configuration (i.e., structure of locations) affects our outcome.

We will focus in this practical on spatial lag and spatial error models. The following code tests for whether a spatially lagged dependent variable or spatial error dependence can improve our model fit. We can test for more things using this code, but for now we stick with these four tests. 

```{r}
spat_dep_test <- lm.LMtests(model1, lw, test=c("LMerr", "LMlag", "RLMerr", "RLMlag")) # Test for spatial dependence
spat_dep_test

```

If we consider all of the tests, we can see that for each test of spatial dependence that they are each statistically significant. This would suggest that each spatial model can benefit our analysis and model fit. If none were significant, then we would use the OLS regression results. 

## Spatial lag model

The first types of spatial models we will consider are those which incorporate a **spatial lag**. These models use variables that are spatially lagged, which means that they calculate measures for each area that characterise (e.g., mean value) their surrounding neighbours. Spatial lags might correspond to the outcome variable or explanatory variables. A spatial lag suggests that the surrounding areas have an influence on the outcome of an area.  

### SLX spatially lagged model

The first spatial regression model we will consider is the **SLX spatial lag model**. SLX here means Spatially Lagged X-variables. We define the model as:

$$
y = X \beta + WX \theta + \epsilon
$$

The equation is a simple extension of the OLS regression equation. Our outcome variable $y$ is a function of our explanatory variables $X$ and their $\beta$ coefficients, a spatial lag coefficient $\theta$ of the $X$ variables based on a spatial weight $W$ and the error term $\epsilon$. The $\beta$ value represents the *direct effect* of an explanatory variable and the $\theta$ value represents the *indirect effect*. An indirect effect is synonymous with a *spillover effect* whereby changes in $x$ in an area have on it's surrounding neighbours based on how $W$ is defined. The spatial lags are exogenous in definition.

To fit the model, we use the R package `spatialreg` which allows us to fit cross-sectional spatial regression models. We re-run the previous analysis of first dose uptake using this spatial model and tidy the output (please note that `gtsummary` does not handle spatial models well).

```{r message=FALSE, warning=FALSE}
library(spatialreg) # Load package
model2 <- lmSLX(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, lw) # Spatial lag model
tbl_model2 <- tbl_regression(model2, label = list(median_age ~ "Median age", Other_White ~ "Other White (%)", Mixed ~ "Mixed (%)", Black ~ "Black (%)", Asian ~ "Asian (%)", Other ~ "Other Ethnicity (%)", mean_imd_score ~ "Deprivation score", pop_density ~ "Population density", lag.median_age ~ "Lag: Median age", lag.Other_White ~ "Lag: Other White (%)", lag.Mixed ~ "Lag: Mixed (%)", lag.Black ~ "Lag: Black (%)", lag.Asian ~ "Lag: Asian (%)", lag.Other ~ "Lag: Other Ethnicity (%)", lag.mean_imd_score ~ "Lag: Deprivation score", lag.pop_density ~ "Lag: Population density")) # Tidy model output
tbl_model2 # Print

```

To interpret the model can be difficult. The $\beta$ coefficients are not exactly the same. Rather, to understand the marginal effect of our covariates, we need to estimate their total impacts (i.e., direct effect + indirect effect). To do this, we use the following piece of code.

```{r}
model2_imp <- impacts(model2, listw = lw) # Estimate direct, indirect and total effects of variables
model2_imp # Print

```

You will see here these are exactly the same as $\beta$ coefficients for this model, however this will not always be the case. What is useful here is that we can see the direct impact in an area, the indirect effects surrounding each and the total effect of each factor considering both together. The spatial lags here are mostly non-statistically significant, other than for population density. It suggests that they bring little to the model.

Standard errors and p-values for these statistics can be estimated through the following modified version of the code.

```{r}
model2_imp_se <- summary(impacts(model2, lw), zstats = TRUE) # Estimate
model2_imp_se # Print

```

We can also compare the results from the spatial model to our original OLS regression model

```{r message=FALSE, warning=FALSE}
tbl_merge(list(tbl_model1, tbl_model2)) # Combine both model outputs together

```

Not a lot has changed if we compare the direct effects of coefficients (many remain within their 95% confidence intervals), probably because the spatial lagged effects were not strong or identifiable. Population density has become non-statistically significant mind you, as has percentage of people who with mixed ethnicity. 

We can also compare model fit to see if the spatial model improves on the original OLS regression model. For example, we can check this by quickly looking at the AIC model fit. 

```{r}
AIC(model1) # OLS regression model
AIC(model2) # SLX spatial lag model

```

The spatial lag model does improve model fit overall. 

### SAR Spatial Lag

The other spatial lag model that we can specify is the following:

$$
y = \rho Wy + X\beta + \epsilon
$$

SAR stands for Spatial AutoRegressive model. Here, we have the standard OLS regression equation $X\beta + \epsilon$, however we also have included a spatial lag component $\rho W$ of our outcome variable $y$. $W$ is once again our spatial weights matrix, with $\rho$ representing the correlation of the spatial lag of $y$ to $y$.

Since the model contains $y$ at both sides of the equation, it violates the exogeneity assumption of the OLS regression model. We therefore need a different approach to estimate the equation. 

Please note: `gtsummary` does not handle spatial models well, so we will have to ~~fudge it~~ persist a bit here. If you can fix it, send your answers on a postcard/carrier pigeon please. Remember, you can always get the raw output of a model by using `summary(model3)`.

```{r message=FALSE, warning=FALSE}
model3 <- lagsarlm(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, lw) # Model
tbl_model3 <- tbl_regression(model3) # Tidy model output
tbl_model3 # Print

```

Let's compare it to our standard OLS regression model. We will go back to the untidy version of the output to match the above

```{r}
tbl_model1 <- tbl_regression(model1, intercept = TRUE) # Redo Table 1 to match above format
tbl_merge(tbls = list(tbl_model1, tbl_model3), # Join output together
          tab_spanner = c("**OLS**", "**Spatial Lag**")) # Rename columns (in bold)

```

Not a lot has changed between the models, suggesting that the spatial patterns captured very little of the patterns observed in our covariates. The most interesting difference is for our population density variable `pop_density`, which has gone from statistically significant in our OLS regression model to insignificant in our spatial lag model. The percentage of people with their ethnicity as other White (`Other_White`) has now become statistically significant as well, with a negative association to the percentage vaccinated. Otherwise, the coefficients have remained similar and within the 95% Confidence Intervals of the OLS estimates.

The summary output for the spatial lag model gives us only the direct impacts for our coefficients, however we may be interested in the direct, indirect and total effects each covariate is having on our outcome variable.

```{r}
impacts(model3, listw = lw) # Estimate direct and indirect effects

```

The table would suggest that the indirect effects of each covariate on the surrounding areas means that focusing on the direct association alone underestimates the total impact of each variable.

To get the standard errors and p-values for our direct, indirect and total effects, we use the following code. Unlike the SLX model, we cannot estimate these directly and therefore must use simulations (MCMC) approaches to estimate them. This only requires a slight modification to the code, in our case we will add in 500 simulations (the more the merrier, but impacts on processing time).

```{r}
summary(impacts(model3, listw = lw, R = 500), zstats = TRUE) # Slow, but gives quantiles etc

```

It gives a lot of output, but it is useful to pick apart how useful our estimates are.

## Spatial error models

We next explore how to implement spatial error models. The underlying models are modified OLS regression models meaning the models are more straightforward ways of accounting for spatial processes in our analysis, as well as being fairly easy to interpret. In this section, we will deal with models that incorporate the spatial structure of the dataset within the error terms. Remember that this was the key OLS assumption that might have been violated (i.e., independence of errors). 

### Spatial Error Model (SEM)

The main equation that we are trying to estimate in our spatial error model is:

$$
y = X\beta + u
$$

Here, we are trying to explain our outcome variable $y$ based upon our the $\beta$ coefficients of covariate variables $X$ plus our error term $u$. The equation itself looks fairly similar to the OLS equation, until we introduce the spatial autocorrelation element to our model. We partition $u$ into the following:

$$
u = \lambda Wu + \epsilon
$$

We define $u$ as a coefficient $\lambda$ controlling the spatial weight $W$ placed on the the error terms $u$, plus the general error term $\epsilon$ for our model.

We cannot use the OLS regression model to estimate the equation, since the assumption of independence of error terms is violated. We therefore require new approaches to do this. Again, `gtsummary` doesn't like these types of models but I have included here as it helps clean the output up.

```{r}
model4 <- errorsarlm(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, lw) # Model
tbl_model4 <- tbl_regression(model4) # Tidy model output
tbl_model4 # Print

```

These models do not have direct and indirect effects since the spatial component only affects the error term. 

We may also want to check whether a spatial error model was relevant here through running a **Hausman Test** which compares if the model improves upon the OLS model.

```{r}
HausmanTest <- Hausman.test(model4) # Run test
HausmanTest # Print

```

The test suggests that the spatial error improves the model (i.e., we want it to be statistically significant).

We can compare the model output to the OLS and spatial lag models too.

```{r message=FALSE, warning=FALSE}
tbl_merge(tbls = list(tbl_model1, tbl_model3, tbl_model4), # Join three model outputs together
          tab_spanner = c("**OLS**", "**Spatial Lag**", "**Spatial Error**")) # Rename columns (in bold)

```

The spatial error model looks relatively similar to the spatial lag model, although there are some differences with `Mixed` and `Asian` variables now being statistically insignificant. 

### Spatial Durbin Error

We can extend the spatial error model through adding in spatial lags for our x-variables, accounting for the spatial effects of independent variables. This requires only a minor modification of the spatial error model equation:

$$
y = X\beta + WX\theta + u
$$
The key difference to the spatial error model is the introduction of $WX\theta$ which applies the spatial weight $W$ to the independent variables $X$ and our spatial autocorrelation coefficient $\theta$.

$u$ again is defined as:

$$
u = \lambda Wu + \epsilon
$$

We can fit this model using a slight change to the spatial error code. Please note that `gtsummary` does not like this type of model output either, so it is a little messy again. 

```{r message=FALSE, warning=FALSE}
model5 <- errorsarlm(percent_first_dose ~ median_age + Other_White + Mixed + Black + Asian + Other + mean_imd_score + pop_density, data = lad_eng, lw, etype = "emixed") # Fit model
tbl_model5 <- tbl_regression(model5) # Tidy model output
tbl_model5 # Print

```

The inclusion of spatial lags does not seem to bring much advantage here. Since we have introduced spatial lags into our model, we can also calculate the direct, indirect and total effects of covariates too. I have included the code for calculating standard errors, however we don't need to run it again ~~because I am lazy~~.

```{r}
impacts(model5, listw = lw) # Calculate indirect, direct and total effects
# summary(impacts(model3, listw = lw, R = 500), zstats = TRUE) # Slow, but gives quantiles, standard errors and p-values

```

There are more extensions of these models ~~than you can shake a stick at~~ we could explore here - for example, SARAR or SARMA - and most of them are essentially different combinations of spatial lags and spatial error models. We can save them for a rainy day.

## Summary

In this practical session, we have explored how to extend the standard OLS regression model to incorporate spatial processes. We will continue this theme in the next, and final, session where we examine Geographically Weighted Regression.

